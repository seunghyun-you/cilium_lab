```bash
$ hubble observe -f --protocol tcp --pod curl-pod
l 26 08:15:33.840: default/curl-pod (ID:37934) <> 10.96.88.194:80 (world) pre-xlate-fwd TRACED (TCP)
Jul 26 08:15:33.840: default/curl-pod (ID:37934) <> default/webpod-697b545f57-2h59t:80 (ID:23913) post-xlate-fwd TRANSLATED (TCP)
Jul 26 08:15:33.840: default/curl-pod:53092 (ID:37934) -> default/webpod-697b545f57-2h59t:80 (ID:23913) to-network FORWARDED (TCP Flags: SYN)
Jul 26 08:15:33.841: default/curl-pod:53092 (ID:37934) <- default/webpod-697b545f57-2h59t:80 (ID:23913) to-endpoint FORWARDED (TCP Flags: SYN, ACK)
# pre-xlate-fwd , TRACED : NAT (IP 변환) 전 , 추적 중인 flow
# post-xlate-fwd , TRANSLATED : NAT 후의 흐름 , NAT 변환이 일어났음
```

ipam migration : kubernetes scope → cilium cluster scope

- operator restart
- cilium agent restart
- cilium node rejoin
- pod ip re allocate (restart)
- hubble-ui process `kill 9- <PID>`

## Routing

- native routing : 상대방 노드에 할당된 pod cidr이 노드의 route table에 자동으로 상대방 노드의 ip로 향하도록 route가 추가됨
- 노드간의 파드가 통신하는데 마스커레이딩이 없이 통신하도록 설정이 추가됨 `ipv4-native-routing-cidr: x.x.x.x/y`

## Masquerading

- 클러스터 내부에서 쓰는 IPv4는 Private IP 이기에 외부로 통신 제한
- 클러스터를 떠나는 모든 트래픽의 소스 IP 주소를 자동으로 masquerade 하여 외부와 통신이 가능하게 해주는 기능
- 노드 간의 통신에는 Encapsulation, Native Routing 방식으로 통신이 되지만 클러스터를 떠나는 경우에 한함 (eBPF 마스커딩이 활성화되면 포드에서 클러스터 노드의 External IP로의 트래픽도 마스커딩 미지원)
- eBPF(default)를 통해서 masquerade하거나 iptables로 masquerade 할 수도 있음
- 정교한 설정은 (Cilium 의 eBPF 구현) ip-masq-agent 를 통해서 가능
- 클러스터 외부 네트워크 중 사내망 처럼 마스커레이딩이 필요 없는 경우 `ip-masq-agent`를 설정해서 NAt 없이 통신이 가능하도록 설정 가능
- helm 설치간에 ip masquerading agent를 활성화 해야하고 그 후 cm이 하나 추가로 생성된다.
- masq agent 생성 옵션 중에 non-masquerading 옵션을 줄 수 있다.
- 그렇지만 cluster 쪽에서만 설정하면 상대방측에서 돌아오는 정보가 없어서 망해버린다. (상대방측에서도 ip routing 정보를 추가해주어야 한다)
- 노드별로 pod cidr 값이 맵핑되어 잇는 정보에 맞춰서 ip routing 정보가 할당되어 있어야 함
- 규모가 작은 경우에는 static routing 설정으로 처리 가능하지만 규모가 큰 경우 BGP를 연동해서 써야 함(동적관리를 위해서)

## 노드 / 파드 간 통신 테스트

- 노드 별로 실행되고 있는 파드 IP 확인

```bash
$ k get po -owide
NAME                      READY   STATUS    RESTARTS   AGE    IP             NODE         NOMINATED NODE   READINESS GATES
curl-pod                  1/1     Running   0          2d8h   172.20.2.61    cilium-ctr   <none>           <none>
webpod-697b545f57-9x2g4   1/1     Running   0          2d8h   172.20.1.242   cilium-w2    <none>           <none>
webpod-697b545f57-g4txg   1/1     Running   0          2d8h   172.20.0.132   cilium-w1    <none>           <none>
```

- WEB POD의 IP 정보를 변수에 할당

```bash
export WEBPODIP1=$(kubectl get -l app=webpod pods --field-selector spec.nodeName=cilium-w1 -o jsonpath='{.items[0].status.podIP}')
export WEBPODIP2=$(kubectl get -l app=webpod pods --field-selector spec.nodeName=cilium-w2 -o jsonpath='{.items[0].status.podIP}')
```

- curl-pod에서 worker node 1번에 떠 있는 webpod로 통신 테스트

```bash
kubectl exec -it curl-pod -- ping $WEBPODIP1
```

- cilium-ctr tcpdump 실행 결과 : pod의 ip가 그대로 출력되는 것을 확인할 수 있음

```bash
$ tcpdump -i eth1 icmp
21:04:55.394770 IP 172.20.2.61 > 172.20.0.132: ICMP echo request, id 54, seq 1, length 64
21:04:55.394994 IP 172.20.0.132 > 172.20.2.61: ICMP echo reply, id 54, seq 1, length 64
21:04:56.412980 IP 172.20.2.61 > 172.20.0.132: ICMP echo request, id 54, seq 2, length 64
21:04:56.413229 IP 172.20.0.132 > 172.20.2.61: ICMP echo reply, id 54, seq 2, length 64
21:04:57.437067 IP 172.20.2.61 > 172.20.0.132: ICMP echo request, id 54, seq 3, length 64
21:04:57.437380 IP 172.20.0.132 > 172.20.2.61: ICMP echo reply, id 54, seq 3, length 64
```

-

## cilium coreDNS

- DNS가 관리하는 메트릭 정보를 확인하는 방법

```
kubectl exec -it curl-pod -- curl kube-dns.kube-system.svc:9153/metrics | grep coredns_cache_ | grep -v ^#
```

### node loacl DNS

- coreDNS의 성능은 고려하는 경우 노드 로컬 DNS(데몬셋)의 Cache 사용을 고려한다.
- 노드로컬DNS는 자신이 CoreDNS에게 전달받은 도메인 정보를 캐시로 들고 있다가 Hit 방식으로 Pod들에게
- 노드로컬DNS는 링크-로컬 범위 사용을 권장 (cilium에서 마스커레이딩 안하는 대역 값)
- 1만개의 정보를 캐시로 사용하는데 30MB 정도의 스펙을 사용함
- cilium은 eBPF로 처리하기 때문에 node local DNS로 향하지 못하는 이슈가 있음
  - node local DNS는 iptables로 라우팅 처리가 되어 잇기 때문에 실제로는 eBPF가 트래픽을 처리하기 때문에 node local의 링크-로컬 로 향하게 하지 못함
  - 별도의 helm options (`localRedirectPolicy=true`)을 같이 활성화해주어야함
  - 추가 후 operator, cilium agent 재실행 필요
  - cilium에서 node local dns yaml이 별도로 제공하기 때문에 해당 매니페스트로 생성하는게 좋음 (원래는 다른 cni에서 사용하던 매니페스트가 있으나 내부 내용이 조금 다름)
