**Table of Contents**
---
- [**Table of Contents**](#table-of-contents)
- [Cilium](#cilium)
  - [1. Cilium 구성요소](#1-cilium-구성요소)
    - [1.1 Cilium 구성요소](#11-cilium-구성요소)
    - [1.2 Cilium 에서 제공하는 다양한 디버깅 툴](#12-cilium-에서-제공하는-다양한-디버깅-툴)
  - [2. 네트워크 구성 정보 : _cilium host, cilium net, cilium health_](#2-네트워크-구성-정보--cilium-host-cilium-net-cilium-health)
    - [2.1 네트워크 인터페이스 구조](#21-네트워크-인터페이스-구조)
    - [2.2 Cilium Network Interface 조회](#22-cilium-network-interface-조회)
  - [3. eBPF를 이용한 Packet Flow (eBPF Datapath)](#3-ebpf를-이용한-packet-flow-ebpf-datapath)
    - [3.1 같은 노드에 배치된 Pod 간의 통신 경로](#31-같은-노드에-배치된-pod-간의-통신-경로)
    - [3.2 Pod에서 외부로 나가는 트래픽 통신 경로](#32-pod에서-외부로-나가는-트래픽-통신-경로)
    - [3.3 Pod 내부로 들어오는 트래픽 통신 경로](#33-pod-내부로-들어오는-트래픽-통신-경로)
    - [3.4 Packet Flow 과정에 사용되는 eBPF Program \& Function 조회 방법](#34-packet-flow-과정에-사용되는-ebpf-program--function-조회-방법)
- [Cilium Networking](#cilium-networking)
  - [1. cilium이 서로 다른 노드에 있는 Pod를 연결하는 두 가지 방법: _Encapsulation(VxLAN, Geneve), Native/Direct_](#1-cilium이-서로-다른-노드에-있는-pod를-연결하는-두-가지-방법-encapsulationvxlan-geneve-nativedirect)
    - [1.1 Encapsulation Routing Mode (Default)](#11-encapsulation-routing-mode-default)
    - [1.2 Native Routing Mode](#12-native-routing-mode)
    - [1.3 Native Routing Mode 파드간 통신 테스트](#13-native-routing-mode-파드간-통신-테스트)
  - [2. 클러스터 외부로 향하는 패킷 처리를 위한 Maquerading 처리](#2-클러스터-외부로-향하는-패킷-처리를-위한-maquerading-처리)
    - [2.1 Cluster와 같은 네트워크에 있지만 Cluster에 Join 되지 않은 VM 과의 통신 테스트](#21-cluster와-같은-네트워크에-있지만-cluster에-join-되지-않은-vm-과의-통신-테스트)
    - [2.2 Cluster 외부의 다른 네트워크 대역(10.0.0.0/16)에 있는 네트워크와의 통신 테스트](#22-cluster-외부의-다른-네트워크-대역1000016에-있는-네트워크와의-통신-테스트)
    - [2.3 Cluster 외부의 다른 네트워크 대역(10.0.0.0/16)과 Masquerade 없이 통신하도록 설정하는 방법](#23-cluster-외부의-다른-네트워크-대역1000016과-masquerade-없이-통신하도록-설정하는-방법)
  - [3. 네트워크 엔드포인트(컨테이너/LB) IP 관리를 위한 IPAM (IP Address Management)](#3-네트워크-엔드포인트컨테이너lb-ip-관리를-위한-ipam-ip-address-management)
    - [3.1 Kubernetes Host Scope](#31-kubernetes-host-scope)
    - [3.2 Cluster Scope IPAM (Default)](#32-cluster-scope-ipam-default)
    - [3.3 Multi Pool](#33-multi-pool)
    - [3.4 Load Balaner / Egress IPAM](#34-load-balaner--egress-ipam)

## Cilium

<p align=center><img src="./_image/cilium.png" title="출처: Getting started with Cilium for Kubernetes networking and observability" width="60%"></p>

- Cilium은 eBPF 기술을 이용해서 쿠버네티스의 네트워크와 보안 기능을 구현한 쿠버네티스의 CNI Plugin 이다.

- eBPF는 리눅스 커널의 소스코드 변경 없이 커널 내부에서 샌드박스 프로그램을 실행시켜 커널의 기능을 효율적으로 확장시킬 수 있다. [ [BLOG](https://zerotay-blog.vercel.app/4.RESOURCE/KNOWLEDGE/OS/eBPF/) ]

<br>

### 1. Cilium 구성요소

#### 1.1 Cilium 구성요소

<p align=center><img src="./_image/cilium_architecture_02.png" title="출처: https://velog.io/@baeyuna97/Cilium" width="50%"></p>

##### Cilium Operator

- Deployment로 배포되어 쿠버네티스 클러스터 수준에서 처리해아 하는 작업(CRD, IPAM 등)을 관리한다.

- Operator는 네트워킹 과정에 깊게 관여하지 않아 일시적인 중단에도 클러스터 동작에 큰 영향을 미치지 않는다.

  - IPAM Pool의 고갈 시 신규 IPAM Pool을 노드에 할당해야 하는데,

  - Operator의 장애로 신규 IPAM Pool 할당이 안될 경우 신규 Pod 생성이 실패한다.

##### Cilium CNI Plug-in (Node Level 작업)

- Pod 생성/삭제/주기적인 상태 확인 시 마다 Container Runtime에 의해 실행되면서 네트워크 구성(가상 인터페이스 구성, IP 할당/해제 등) 작업을 수행한다.

- Operator, Agent, Envoy와 같이 컨테이너로 동작하지 않고 필요할 때만 Container Runtime에 의해 실행된다.

- Binary 파일(`/opt/cni/bin/cilium-cni`, `/etc/cni/net.d/05-cilium.conflist`)로 각 노드에서 관리된다.

##### Cilium Agent (Kernel Level 작업, L3-4 계층)

- 데몬셋(DeamonSet)으로 배포되어 각 노드에서 Pod로 실행된다.

- Cilium Agent는 eBPF 로드, Cilium CNI Plug-in 설치, 쿠버네티스 상태 모니터링을 수행한다.

  - eBPF Program과 MAP은 Kernel에 Load 하는데 이 프로그램을 통해 네트워킹, 네트워크 정책 시행, 트래픽 리다이렉트, 서비스 부하분산 등 네트워크 기능을 수행한다.

  - Cilium CNI Plug-in Binary 파일을 노드에 설치하여 전반적인 네트워크 구성의 기능을 제공한다.

  - API Server와 통신하면서 쿠버네티스 리소스의 상태를 모니터링한다. 변경 사항이 발생하면 eBPF MAP을 업데이트 한다.

##### Envoy Proxy (UserSpace 작업, L7 계층)

- 데몬셋(DeamonSet)으로 배포되어 각 노드에서 Pod로 실행된다.

- Cilium L7 계층 관련 기능(Ingress, Gateway API, L7 Network Policies 등)을 사용하는 경우, Envoy Proxy를 통해 라우팅된다.

- HTTP 트래픽 제어, L7 Network Policy 시행, 정교한 라우팅 정책 관리와 같은 L7 기반 동적 라우팅/보안 기능을 제공한다.

> [!TIP]
> - Cilium은 L3~4 계층의 트래픽은 Kernel의 eBPF Program으로 관리하고, L7 관련 트래픽 처리는 Envoy에 위임는 구조로 동작한다.
> - 이러한 구조로 인해 MetalLB의 지원 없이 직접 LB, Ingress의 External IP 할당을 관리할 수 있다.
> - Envoy는 Ingress 라우팅 규칙 관리, L7 계층 Network Policy 등을 구현하고, Ingress의 External IP에 대한 L2 announcement, BGP 처리는 eBPF에서 처리한다.

<br>

#### 1.2 Cilium 에서 제공하는 다양한 디버깅 툴

- Cilium Agent에는 관리용 CLI 두 가지(`cilium`, `bpftool`)를 제공한다.

  - cilium(cilium-dbg) : Cilium 구성과 관련된 일반 적인 상태 정보, 정책, 서비스 목록 등을 확인하는데 사용한다.

  - bpftool : eBPF 수준의 디버깅이 필요한 경우 사용되는 도구로 커널에 로드된 eBPF 프로그램 목록, MAP 목록 조회 등에 사용한다.

- Cilium Agent Pod 내에서 실행할 수 있지만 `kubectl` 명령을 사용할 수 있는 노드(Control)에서 각 노드의 Cilium Agent를 통해 툴을 사용하도록 단축키를 지정한다.

  ```bash
  # /ect/profile
  export CILIUMPOD0=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=cilium-ctr -o jsonpath='{.items[0].metadata.name}')
  export CILIUMPOD1=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=cilium-w1 -o jsonpath='{.items[0].metadata.name}')
  export CILIUMPOD2=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=cilium-w2 -o jsonpath='{.items[0].metadata.name}')
  alias c0="kubectl exec -it $CILIUMPOD0 -n kube-system -c cilium-agent -- cilium"
  alias c1="kubectl exec -it $CILIUMPOD1 -n kube-system -c cilium-agent -- cilium"
  alias c2="kubectl exec -it $CILIUMPOD2 -n kube-system -c cilium-agent -- cilium"
  alias c0bpf="kubectl exec -it $CILIUMPOD0 -n kube-system -c cilium-agent -- bpftool"
  alias c1bpf="kubectl exec -it $CILIUMPOD1 -n kube-system -c cilium-agent -- bpftool"
  alias c2bpf="kubectl exec -it $CILIUMPOD2 -n kube-system -c cilium-agent -- bpftool"
  ```

  ```bash
  source /etc/profile
  ```

<br>

### 2. 네트워크 구성 정보 : _<span style="font-size: smaller; color: Aquamarine;">cilium host, cilium net, cilium health</span>_

#### 2.1 네트워크 인터페이스 구조

<p align=center><img src="./_image/cilium_interface.png" title="출처: Ctrip Network Architecture Evolution in the Cloud Computing Era" width="40%"></p>

##### cilium_host

- Cilium이 관리하는 호스트 네트워크 인터페이스

- 클러스터 내의 Pod와 외부 네트워크 간의 연결을 처리 (Pod가 외부 네트워크와 통신할 때 이 인터페이스 사용)

- Pod에서 나가는 트래픽과 외부에서 들어오는 트래픽을 처리하는 데 사용

- 외부에서 들어오는 패킷을 Pod로 전달하는 Reverse NAT 기능을 포함

##### cilium_net

- Cilium에서 관리하는 가상 네트워크 인터페이스

- Cilium의 eBPF 프로그램을 통해 네트워크 트래픽을 처리하고 제어

- Pod 간 통신 관리(보안 정책 적용, 패킷 필터링, 네트워크 성능 측정 등)를 위해 cilium_net 사용

- cilium_net 인터페이스는 각 Pod에 대해 생성되며, Pod의 IP 주소와 연결

- 이 인터페이스는 eBPF 프로그램이 패킷을 검사하고 처리할 수 있도록 해준다.

##### cilium_health (lxc_health)

- 컨테이너의 상태 확인에 사용하는 인터페이스

##### lxcxxxx

- 컨테이너에 할당되는 eth 인터페이스와 호스트의 인터페이스와 맵핑되는 가상 인터페이스

<br>

#### 2.2 Cilium Network Interface 조회

- Control 노드 cilium net과 cilium host 정보 조회

  - control 노드에서 인터페이스의 목록을 확인해보면 cilium net, cilium host, lxc_health 인터페이스를 볼 수 있다.

    ```bash
    $ ip -c -br addr show
    eth0                    UP             10.0.2.15/24 metric 100 fd17:625c:f037:2:a00:27ff:fe6b:69c9/64 fe80::a00:27ff:fe6b:69c9/64 
    eth1                    UP             192.168.10.100/24 fe80::a00:27ff:febd:9813/64 
    cilium_net@cilium_host  UP             fe80::18f2:6aff:fe9d:4953/64 
    cilium_host@cilium_net  UP             172.20.0.6/32 fe80::1cb0:76ff:fe52:2aa/64 
    lxc_health@if6          UP             fe80::7c46:c0ff:fe22:317f/64 
    lxce588bd206718@if8     UP             fe80::ccb:27ff:fe93:8b39/64 
    lxcd4c5e765631e@if10    UP             fe80::2ca3:f9ff:fe4b:520/64 
    ```

- lxc_health Interface 정보 조회

  - lxc_health 인터페이스 같은 경우 Control 노드에서는 IP 정보를 직접 확인할 수 없다.

  - cilium-dbg cli을 통해 control 노드에 할당되어 있는 IP의 정보를 검색하면 `health` 항목에 할당된 IP를 확인할 수 있다.

    ```bash
    $ c0dbg status --verbose | grep -A6 "Allocated"
    Allocated addresses:
      172.20.0.11 (health)
      172.20.0.12 (kube-system/coredns-674b8bbfcf-r4kx2)
      172.20.0.3 (kube-system/coredns-674b8bbfcf-7ltmp)
      172.20.0.6 (router)
    ```

  - cilium에서는 각 endpoint(interface or ip address) 마다 endpoint id를 할당하는데 해당 정보를 cilium cli로 조회한다.

    ```bash
    $ c0 endpoint list
    ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                                  IPv6   IPv4          STATUS   
              ENFORCEMENT        ENFORCEMENT                                                                                                                    
    ...
    1364       Disabled           Disabled          4          reserved:health                                                                     172.20.0.11   ready   
    ...
    ```

  - 172.20.0.11(health) IP에 할당 된 ENDPOINT ID 1364를 이용해서 해당 IP의 세부 정보를 조회하면 Cilium 노드에서 보았던 Interface Name을 확인할 수 있다.

    ```bash
    $ c0 endpoint get 1364 | grep -A11 networking
          "networking": {
            "addressing": [
              {
                "ipv4": "172.20.0.11",        # health interface ip
                "ipv4-pool-name": "default"
              }
            ],
            "host-mac": "7e:46:c0:22:31:7f",  # host mac 정보
            "interface-index": 7, 
            "interface-name": "lxc_health",   # inteface name
            "mac": "3a:c9:b4:f3:4c:9a"
          },
    ```

  - 조금 더 확실하게 비교하기 위해 `host-mac` 정보까지 control node에서 검색해 보면 동일한 정보가 할당되어 있음을 알 수 있다.

    ```bash
    $ ip -c addr show lxc_health
    7: lxc_health@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
        link/ether 7e:46:c0:22:31:7f brd ff:ff:ff:ff:ff:ff link-netnsid 0
        inet6 fe80::7c46:c0ff:fe22:317f/64 scope link 
          valid_lft forever preferred_lft forever
    ```

- lxc_health 외 lxc로 시작하는 기타 인터페이스의 정보 조회

  - cilium 에서는 노드에 Pod가 생성되면 Pod의 인터페이스와 바인딩할 `lxcxxxx` 형태의 인터페이스를 호스트 노드에 생성한다.

  - cilium-dbg cli을 통해 control 노드에 할당되어 있는 IP의 정보를 검색하면 `coredns` Pod에 할당된 IP를 확인할 수 있다.

    ```bash
    $ c0dbg status --verbose | grep -A6 "Allocated"
    Allocated addresses:
      172.20.0.11 (health)
      172.20.0.12 (kube-system/coredns-674b8bbfcf-r4kx2)
      172.20.0.3 (kube-system/coredns-674b8bbfcf-7ltmp)
      172.20.0.6 (router)
    ```

  - coredns의 ENDPOINT ID 값을 조회한 후 해당 ENDPOINT의 상세 내용을 검색하면, 호스트 노드에서 보이는 인터페이스 이름과 일치하는 정보가 확인된다.

    ```bash
    c0 endpoint list
    ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                                  IPv6   IPv4          STATUS   
              ENFORCEMENT        ENFORCEMENT                                                                                                                    
    235        Disabled           Disabled          5118       k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=kube-system          172.20.0.3    ready   
                                                              k8s:io.cilium.k8s.policy.cluster=default                                                                  
                                                              k8s:io.cilium.k8s.policy.serviceaccount=coredns                                                           
                                                              k8s:io.kubernetes.pod.namespace=kube-system                                                               
                                                              k8s:k8s-app=kube-dns 
    2089       Disabled           Disabled          5118       k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=kube-system          172.20.0.12   ready   
                                                              k8s:io.cilium.k8s.policy.cluster=default                                                                  
                                                              k8s:io.cilium.k8s.policy.serviceaccount=coredns                                                           
                                                              k8s:io.kubernetes.pod.namespace=kube-system                                                               
                                                              k8s:k8s-app=kube-dns 
    ```

    ```bash
    c0 endpoint get 235 | grep -A12 networking
          "networking": {
            "addressing": [
              {
                "ipv4": "172.20.0.3",
                "ipv4-pool-name": "default"
              }
            ],
            "container-interface-name": "eth0",
            "host-mac": "2e:a3:f9:4b:05:20",
            "interface-index": 11,
            "interface-name": "lxcd4c5e765631e",
            "mac": "86:0a:05:59:44:d9"
          },
    ```

<br>

### 3. eBPF를 이용한 Packet Flow (eBPF Datapath)

- `L7 Network Policy`, `Encryption`, `Network Mode` 설정이 적용된 경우 점선으로 표시된 항목들까지 통신 경로에 추가 된다.

- 별도의 L7 정책이 없다면 Pod간의 통신에는 `NIC(lxcxxxx)` 인터페이스를 지나는 순간 커널의 TCX(Traffic Control eXpress) Hook 을 통해 eBPF 프로그램(`bpf_lxc`, `bpf_host`)이 트리거 된다.

- 이 때 네트워크 트래픽은 커널 내부의 eBPF 프로그램에 의해서 정책이 평가되고, 평가 결과에 따라서 Flow가 결정된다.

#### 3.1 같은 노드에 배치된 Pod 간의 통신 경로

<p align=center><img src="./_image/packet_flow_endpoint_to_endpoint.png" title="출처: Cilium Documentation - Networking.eBPF_Datapath.Life_of_a_Packet" width="90%"></p>

<br>

#### 3.2 Pod에서 외부로 나가는 트래픽 통신 경로

<p align=center><img src="./_image/packet_flow_egress_from_endpoint.png" title="출처: Cilium Documentation - Networking.eBPF_Datapath.Life_of_a_Packet" width="90%"></p>

<br>

#### 3.3 Pod 내부로 들어오는 트래픽 통신 경로

<p align=center><img src="./_image/packet_flow_inress_from_endpoint.png" title="출처: Cilium Documentation - Networking.eBPF_Datapath.Life_of_a_Packet" width="90%"></p>

<br>

#### 3.4 Packet Flow 과정에 사용되는 eBPF Program & Function 조회 방법


- 실행되는 eBPF 프로그램(Component)명은 노란색 박스에 표시되는데, 이 프로그램들은 `cilium-agent` 내부에 C언어 파일로 `/var/lib/cilium/bpf` 폴더에 존재한다.
  
- 이 코드에 대한 내용은 Cilium GitHub 문서에서도 확인할 수 있다. [ [docs](https://github.com/cilium/cilium/blob/main/bpf/bpf_lxc.c) ]

  ```bash
  # /var/lib/cilium/bpf
  $ ls -al
  total 336
  drwxr-xr-x 1 root root  4096 Jul 16 10:05 .
  drwxr-x--- 1 root root  4096 Jul 28 14:05 ..
  -rw-r--r-- 1 root root   420 Jul 16 10:05 COPYING
  -rw-r--r-- 1 root root  1296 Jul 16 10:05 LICENSE.BSD-2-Clause
  -rw-r--r-- 1 root root 18012 Jul 16 10:05 LICENSE.GPL-2.0
  -rw-r--r-- 1 root root 20261 Jul 16 10:05 Makefile
  -rw-r--r-- 1 root root  3533 Jul 16 10:05 Makefile.bpf
  -rw-r--r-- 1 root root  2945 Jul 16 10:05 bpf_alignchecker.c
  -rw-r--r-- 1 root root 58666 Jul 16 10:05 bpf_host.c
  -rw-r--r-- 1 root root 76153 Jul 16 10:05 bpf_lxc.c       # lxcxxx interface를 지날 때 트리거 되는 eBPF 프로그램
  -rw-r--r-- 1 root root  2797 Jul 16 10:05 bpf_network.c
  -rw-r--r-- 1 root root 25289 Jul 16 10:05 bpf_overlay.c
  -rw-r--r-- 1 root root 31334 Jul 16 10:05 bpf_sock.c
  -rw-r--r-- 1 root root  1424 Jul 16 10:05 bpf_wireguard.c
  -rw-r--r-- 1 root root  9064 Jul 16 10:05 bpf_xdp.c
  drwxr-xr-x 6 root root  4096 Jul 16 10:05 complexity-tests
  drwxr-xr-x 2 root root  4096 Jul 16 10:05 custom
  -rw-r--r-- 1 root root  1870 Jul 16 10:05 ep_config.h
  -rw-r--r-- 1 root root   517 Jul 16 10:05 filter_config.h
  drwxr-xr-x 1 root root  4096 Jul 16 10:05 include
  drwxr-xr-x 2 root root  4096 Jul 16 10:05 lib
  -rw-r--r-- 1 root root   404 Jul 16 10:05 netdev_config.h
  -rw-r--r-- 1 root root 10753 Jul 16 10:05 node_config.h
  drwxr-xr-x 4 root root  4096 Jul 16 10:05 tests
  ```

- Pod의 인터페이스 마다 할당되는 eBPF 프로그램의 함수는 `Pod Interface 정보 조회` → `할당된 eBPF Program ID 조회` → `프로그램 내 함수명 조회` 순으로 확인할 수 있다.

  - Pod Interface 정보 조회

    - Pod 조회

      ```bash
      $ kubectl get po -owide
      NAME                      READY   STATUS    RESTARTS        AGE   IP             NODE         NOMINATED NODE   READINESS GATES
      curl-pod                  1/1     Running   1 (4h34m ago)   14h   172.20.2.244   cilium-ctr   <none>           <none>
      webpod-697b545f57-5fmfp   1/1     Running   0               14h   172.20.0.97    cilium-w1    <none>           <none>
      webpod-697b545f57-g9c5f   1/1     Running   0               14h   172.20.1.38    cilium-w2    <none>           <none>
      ```

    - Pod의 ENDPOINT 정보 조회

      ```bash
      $ c0 endpoint list | grep 172.20.2.244
      ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])        IPv6   IPv4           STATUS   
                ENFORCEMENT        ENFORCEMENT
      4          Disabled           Disabled          47080      k8s:app=curl                              172.20.2.244   ready
      ```

    - Pod의 Interface Name 조회

      ```bash
      $ c0 endpoint get 4 | grep -A11 networking
            "networking": {
              "addressing": [
                {
                  "ipv4": "172.20.2.244",
                  "ipv4-pool-name": "default"
                }
              ],
              "container-interface-name": "eth0",
              "host-mac": "12:73:95:0d:4e:56",
              "interface-index": 9,
              "interface-name": "lxc49adfa975abf",
              "mac": "4e:22:88:88:7d:20"
      ```

  - Pod의 Interface에 적용된 eBPF 프로그램을 조회하면 `cil_from_container`, `cil_to_container` 함수 이름을 확인할 수 있다.

    ```bash
    $ c0bpf net show | grep lxc49adfa975abf
    lxc49adfa975abf(9) tcx/ingress cil_from_container prog_id 1143 link_id 23 
    lxc49adfa975abf(9) tcx/egress cil_to_container prog_id 1149 link_id 24
    ```

- 이 함수는 bpf_lxc.c 파일에 선언되어 있는데 ciliu-agent에 접속 후 해당 함수(`cil_to_container`)를 조회하면 함수의 세부 코드를 확인할 수 있다.

  ```bash
  $ grep -n -A160 "cil_to_container" /var/lib/cilium/bpf/bpf_lxc.c
  2335:int cil_to_container(struct __ctx_buff *ctx)
  2336-{
  2337-   enum trace_point trace = TRACE_FROM_STACK;
  2338-   __u32 magic, identity = 0;
  2339-   __u32 sec_label = SECLABEL;
  2340-   __s8 ext_err = 0;
  2341-   __u16 proto;
  2342-   int ret;
  2343-
  2344-   if (!validate_ethertype(ctx, &proto)) {
  2345-           ret = DROP_UNSUPPORTED_L2;
  2346-           goto out;
  2347-   }
  2348-
  2349-   bpf_clear_meta(ctx);
  2350-
  ...
  2442-
  2443-out:
  2444-   if (IS_ERR(ret))
  2445-           return send_drop_notify_ext(ctx, identity, sec_label, LXC_ID, ret,
  2446-                                       ext_err, CTX_ACT_DROP, METRIC_INGRESS);
  2447-
  2448-   return ret;
  2449-}
  2450-
  2451-BPF_LICENSE("Dual BSD/GPL");
  ```

  - 함수 시작과 동시에 `trace`, `magic`, `identity`, `sec_label` 과 같은 변수를 초기화 한다.
  
  - `validate_ethertype()` 함수를 통해 Ethernet Frame의 Type을 확인하고, 유형이 맞지 않으면 DROP 시키는 코드를 반환한다.
  
  - `bpf_clear_meta()`함수로 전달 받은 패킷 컨텍스트를 초기화 후 새로운 정책 적용을 위한 준비를 한다.
  
  - `EBABLE_L7_LB` 가 정의되어 있는 경우 해당하는 정책으로 점프 시킨다.
  
  - `inherit_identity_from_host()` 함수를 통해 호스트의 정보를 가져와 `MARK_MAGIC_PROXY_INGRESS`, `MARK_MAGIC_PROXY_EGRESS` 인 경우 `send_trace_notify()` 함수를 실행한다.
  
  - 호스트의 방화벽이 활성화되어 있는 경우 `tail_call_policy()` 함수를 통해 호스트 방화벽 정책 적용 후 bpf_lxc로 돌아와 결과에 따라서 패킷을 처리한다.
  
  - Protocol의 종류(ARP, IPv4, IPv6)에 따라서 `tail_call_internal()`함수로 처리하거나 `CTX_ACT_OK` 값을 리턴하도록 구성되어 있다.

<br>

## Cilium Networking

- 기존의 전통적인(Standard) 방식의 CNI 기능은 `kube-proxy(iptables)`를 기반으로 동작한다.

- Cilium에서는 "kube-proxy 대체 모드"를 사용하면 kube-proxy 없이도 클러스터 네트워킹을 구현할 수 있다.

- eBPF 기반의 Cilium CNI는 `kube-proxy(iptables)`를 사용하는 환경보다 더 좋은 성능을 보여준다. [ [link](./_docs/Appendix%201.%20What's%20wrong%20with%20legacy%20iptables.md) ]

<br>

### 1. cilium이 서로 다른 노드에 있는 Pod를 연결하는 두 가지 방법: _<span style="font-size: smaller; color: Aquamarine;">Encapsulation(VxLAN, Geneve), Native/Direct</span>_

#### 1.1 Encapsulation Routing Mode (Default)

<p align=center><img src="./_image/encapsulation_routing_mode.png" title="출처: Kubernetes Networking & Cilium for Network Engineers - An Instruction Manual" width="70%"></p>

- UDP 기반 캡슐화 프로토콜인 VXLAN 또는 Geneve를 사용하여 모든 노드 간에 터널 메시가 생성된다. [ [link](./_docs/Appendix%202.%20VxLAN%20and%20Geneve.md) ]

- 노드 간 통신 트래픽은 모두 VXLAN 또는 Geneve을 통해서 캡슐화된다.

- Pod 네트워크는 노드 네트워크의 영향을 받지 않기 때문에 환경에 종속되지 않고 간단하게 구성할 수 있는 장점이 있다.

- 캡슐화를 통해 헤더가 추가되면서 패킷의 효율이 미미하게 떨어지는데, 최적의 네트워크 성능 보장이 필요한 경우 Native/Direct 모드가 적합하다.

<br>

#### 1.2 Native Routing Mode

<p align=center><img src="./_image/native_routing_mode.png" title="출처: Kubernetes Networking & Cilium for Network Engineers - An Instruction Manual" width="70%"></p>

- 캡슐화 기능 대신 Cilium의 네트워크 기능과 리눅스 커널의 라우팅 시스템을 이용해서 통신한다.

- 각 노드에는 Cilium Agent가 구성되고, Agent는 해당 노드의 Pod들에 대한 네트워크만 관리한다.

- 따라서 다른 노드로 향하는 트래픽은 리눅스 커널의 라우팅 시스템에 위임하여 처리된다.

  - 각 노드는 서로 다른 노드에 할당된 PodCIDR 정보를 인식하고, 커널 라우팅 테이블에 해당 노드로 향하는 경로가 추가된다.

  - L2 네트워크 프로토콜을 사용하는 경우 `auto-direct-node-routes` 옵션을 활성화하여 구성할 수 있다.

  - 또는, BGP 데몬을 활성화한 다음 서로의 라우팅 경로를 배포하도록 구성해야 한다.

<br>

#### 1.3 Native Routing Mode 파드간 통신 테스트


- 통신 테스트를 위해 샘플 파드(curl / netshoot)를 생성한다.

  ```bash
  $ kubectl get po -owide
  NAME                      READY   STATUS    RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES
  curl-pod                  1/1     Running   0          2d23h   172.20.2.61    cilium-ctr   <none>           <none>
  netshoot-pod-w1           1/1     Running   0          55s     172.20.0.81    cilium-w1    <none>           <none>
  netshoot-pod-w2           1/1     Running   0          36s     172.20.1.174   cilium-w2    <none>           <none>
  ```

- WEB-POD의 IP를 변수에 저장 후 활용한다.

  ```bash
  export NETPODIP1=$(kubectl get -l app=curl pods --field-selector spec.nodeName=cilium-w1 -o jsonpath='{.items[0].status.podIP}')
  export NETPODIP2=$(kubectl get -l app=curl pods --field-selector spec.nodeName=cilium-w2 -o jsonpath='{.items[0].status.podIP}')
  ```

  ```bash
  $ echo $NETPODIP1
  172.20.0.81
  $ echo $NETPODIP2
  172.20.1.174
  ```

- `curl-pod`에서 `webpod`로 ICMP 패킷을 전달하고 Pod, Node에서 tcpdumpe를 확인하면 Pod IP로 통신하고 있는 것을 확인할 수 있다.

  ```bash
  kubectl exec -it curl-pod -- ping $NETPODIP1
  ```

  ```bash
  # cilium-ctr 노드에서 실행 한 tcpdump 결과 값
  $ tcpdump -i eth1 icmp
  11:47:48.295596 IP 172.20.2.61 > 172.20.0.81: ICMP echo request, id 96, seq 1, length 64
  11:47:48.295921 IP 172.20.0.81 > 172.20.2.61: ICMP echo reply, id 96, seq 1, length 64
  ...
  # cilium-w1 노드에서 실행 한 tcpdump 결과 값
  $ tcpdump -i eth1 icmp
  11:48:33.992812 IP 172.20.2.61 > 172.20.0.81: ICMP echo request, id 102, seq 1, length 64
  11:48:33.992915 IP 172.20.0.81 > 172.20.2.61: ICMP echo reply, id 102, seq 1, length 64
  ...
  # curl-pod에서 실행 한 tcpdump 결과 값
  $ kubectl exec -it curl-pod -- tcpdump -i eth0 icmp
  02:49:06.529171 IP curl-pod > 172.20.0.81: ICMP echo request, id 114, seq 1, length 64
  02:49:06.529437 IP 172.20.0.81 > curl-pod: ICMP echo reply, id 114, seq 1, length 64
  ...
  # netshoot-pod-w1에서 실행 한 tcpdump 결과 값
  $ kubectl exec -it netshoot-pod-w1 -- tcpdump -i eth0 icmp
  02:49:38.379248 IP 172.20.2.61 > netshoot-pod-w1: ICMP echo request, id 120, seq 1, length 64
  02:49:38.379321 IP netshoot-pod-w1 > 172.20.2.61: ICMP echo reply, id 120, seq 1, length 64
  ```

- 통신 경로를 살펴 보면 각 Pod에서는 자신의 노드의 `cilium_host` ip가 default routing 경로로 설정되어 있다.

  ```bash
  # curl-pod routing table
  $ kubectl exec -it curl-pod -- ip -c route
  default via 172.20.2.225 dev eth0 mtu 1500 
  172.20.2.225 dev eth0 scope link
  # netshoot-pod-w1 routing table
  $ kubectl exec -it netshoot-pod-w1 -- ip -c route
  default via 172.20.0.51 dev eth0 mtu 1500 
  172.20.0.51 dev eth0 scope link 
  ```

  ```bash
  $ for i in ctr w1 ; do echo ">> node : cilium-$i <<"; sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@cilium-$i ip -c addr  | grep -A5 cilium_host@cilium_net ; 
  >> node : cilium-ctr <<
  5: cilium_host@cilium_net: <BROADCAST,MULTICAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
      link/ether 56:e4:89:84:db:22 brd ff:ff:ff:ff:ff:ff
      inet 172.20.2.225/32 scope global cilium_host
        valid_lft forever preferred_lft forever
      inet6 fe80::54e4:89ff:fe84:db22/64 scope link 
        valid_lft forever preferred_lft forever

  >> node : cilium-w1 <<
  5: cilium_host@cilium_net: <BROADCAST,MULTICAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
      link/ether 2a:36:dc:ae:dd:c3 brd ff:ff:ff:ff:ff:ff
      inet 172.20.0.51/32 scope global cilium_host
        valid_lft forever preferred_lft forever
      inet6 fe80::2836:dcff:feae:ddc3/64 scope link 
        valid_lft forever preferred_lft forever
  ```

- `cilium-ctr`과 `cilium-w1` 노드에서 각 파드로 향하는 패킷은 리눅스 커널 라우팅 정보를 기반으로 동작한다.

- 각 노드에 할당된 IP와 노드마다 할당된 PodCIDR, Routing 정보를 확인하면 다음과 같고, `cilium_host` 인터페이스로 전달된 Pod의 패킷이 이 정보를 기반으로 Direct로 전달되는 것을 알 수 있다.

  ```bash
  # cilium node ip information
  $ kubectl get ciliumnodes
  NAME         CILIUMINTERNALIP   INTERNALIP       AGE
  cilium-ctr   172.20.2.225       192.168.10.100   5d14h
  cilium-w1    172.20.0.51        192.168.10.101   5d14h
  cilium-w2    172.20.1.221       192.168.10.102   5d14h
  # cilium-ctr, cilium-w1 PodCIDR information
  $ kubectl get ciliumnode -o jsonpath='{range .items[*]}{.metadata.name}{" "}{.spec.ipam.podCIDRs[*]}{"\n"}{end}' | column -t
  cilium-ctr  172.20.2.0/24
  cilium-w1   172.20.0.0/24
  cilium-w2   172.20.1.0/24
  # cilium-ctr, ciliu,-w1 routeing table 
  $ for i in ctr w1 ; do echo ">> node : cilium-$i <<"; sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@cilium-$i ip -c route show ; echo; done
  ne
  >> node : cilium-ctr <<
  default via 10.0.2.2 dev eth0 proto dhcp src 10.0.2.15 metric 100 
  10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100 
  10.0.2.2 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
  10.0.2.3 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
  172.20.0.0/24 via 192.168.10.101 dev eth1 proto kernel 
  172.20.1.0/24 via 192.168.10.102 dev eth1 proto kernel 
  172.20.2.27 dev lxcf18fa3ec10e8 proto kernel scope link 
  172.20.2.61 dev lxceef064c53fb6 proto kernel scope link 
  172.20.2.106 dev lxc811fcf88a445 proto kernel scope link 
  192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.100 

  >> node : cilium-w1 <<
  default via 10.0.2.2 dev eth0 proto dhcp src 10.0.2.15 metric 100 
  10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100 
  10.0.2.2 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
  10.0.2.3 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
  172.20.0.65 dev lxcc4ed30f5773e proto kernel scope link 
  172.20.0.81 dev lxc50169b2b208e proto kernel scope link 
  172.20.0.106 dev lxc773f942306ed proto kernel scope link 
  172.20.0.218 dev lxc6bc61689249b proto kernel scope link 
  172.20.1.0/24 via 192.168.10.102 dev eth1 proto kernel 
  172.20.2.0/24 via 192.168.10.100 dev eth1 proto kernel 
  192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.101
  ```

- `cilium-ctr` 노드에서 tcpdump 내용을 pcap 파일로 저장 후 정보를 확인하면 자신의 인터페이스와 `cilium-w1` 노드의 인터페이스의 MAC Address를 Source와 Destination 주소로 사용한다.

- `cilium-ctr`, `cilium-w1` 노드에서 각각 interface, arp cache table 정보를 조회해보면 서로 통신에 사용하는 같은 네트워크 `192.168.10.0/24`대역에 할당된 IP, MAC 주소임이 확인된다.

  ![alt text](./_image/termshark_icmp_curlpod_to_netshoot_w1.png)

  ```bash
  # cilium-ctr, cilium-w1 node macc address information
  $ for i in ctr w1 ; do echo ">> node : cilium-$i <<"; sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@cilium-$i ip -c addr | grep -A3 eth1 ; echo; done
  >> node : cilium-ctr <<
  3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
      link/ether 08:00:27:c1:c7:6b brd ff:ff:ff:ff:ff:ff
      altname enp0s8
      inet 192.168.10.100/24 brd 192.168.10.255 scope global eth1
        valid_lft forever preferred_lft forever
      inet6 fe80::a00:27ff:fec1:c76b/64 scope link 
        valid_lft forever preferred_lft forever

  >> node : cilium-w1 <<
  3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
      link/ether 08:00:27:b0:f5:c2 brd ff:ff:ff:ff:ff:ff
      altname enp0s8
      inet 192.168.10.101/24 brd 192.168.10.255 scope global eth1
        valid_lft forever preferred_lft forever
      inet6 fe80::a00:27ff:feb0:f5c2/64 scope link 
        valid_lft forever preferred_lft forever
  ```

  ```bash
  # cilium-ctr, cilum-w1 arp table information
  $ for i in ctr w1 ; do echo ">> node : cilium-$i <<"; sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@cilium-$i arp -a ; echo; done
  >> node : cilium-ctr <<
  cilium-w1 (192.168.10.101) at 08:00:27:b0:f5:c2 [ether] on eth1
  ? (10.0.2.3) at 52:55:0a:00:02:03 [ether] on eth0
  Workspace (192.168.10.1) at 0a:00:27:00:00:00 [ether] on eth1
  ? (172.20.2.27) at 56:dc:67:22:28:5f [ether] on lxcf18fa3ec10e8
  cilium-w2 (192.168.10.102) at 08:00:27:74:3f:da [ether] on eth1
  _gateway (10.0.2.2) at 52:55:0a:00:02:02 [ether] on eth0
  ? (172.20.2.106) at 2e:41:42:6f:af:8b [ether] on lxc811fcf88a445

  >> node : cilium-w1 <<
  Workspace (192.168.10.1) at 0a:00:27:00:00:00 [ether] on eth1
  ? (192.168.10.100) at 08:00:27:c1:c7:6b [ether] on eth1       
  ? (192.168.10.102) at 08:00:27:74:3f:da [ether] on eth1
  ? (172.20.0.218) at f6:95:fa:fc:81:2b [ether] on lxc6bc61689249b
  ? (172.20.0.65) at ce:d6:39:bf:db:a1 [ether] on lxcc4ed30f5773e
  _gateway (10.0.2.2) at 52:55:0a:00:02:02 [ether] on eth0
  ? (10.0.2.3) at 52:55:0a:00:02:03 [ether] on eth0
  ```

- 동작과정 분석 결과 네트워크 처리 과정을 살펴보면 같은 노드에 배치된 파드간의 통신에는 eBPF를 사용해 Cilium이 처리하고, 노드간의 파드 통신에는 리눅스의 라우팅 시스템을 통해 통신한다는 것을 알 수 있다.

<br>

### 2. 클러스터 외부로 향하는 패킷 처리를 위한 Maquerading 처리

<p align=center><img src="./_image/cilium_masquerade.png" title="출처: ISOVALENT_BLOG" width="60%"></p>

- Cluster 내부의 IP CIDR은 RFC1918 Private Address Block을 사용하기 때문에 그 자체만으로는 Public 통신이 제한된다.

- Node IP 주소를 이용해 Cluster 외부로 나가는 모든 트래픽의 Source IP를 자동으로 Masquerade 한다.

- 노드 간의 통신에는 Encapsulation, Native Routing 방식으로 통신하면서 Cluster를 떠나는 트래픽에 대한 Masquerade 만 지원된다.

- eBPF(default) 방식과 iptablse masquerade 방식을 지원한다.

- Cluster 외부 네트워크 중에 사내망의 특별 IP 대역 처럼 Masquerading 없이 통신하고자 하는 경우 `ip-masq-agent` 설정을 통해 Node IP로의 NAT 없이 통신할 수 있다.

#### 2.1 Cluster와 같은 네트워크에 있지만 Cluster에 Join 되지 않은 VM 과의 통신 테스트

- `route VM`을 추가한 다음 Cluster 구성에 사용된 Node와 같은 IP CIDR 대역을 인터페이스에 할당했다.

- Cluster의 `curl-pod`에서 `route VM`(192.168.10.200) 방향으로 ICMP 패킷을 보내면 `curl-pod`가 배포된 노드의 IP로 변환되어 tcpdump에 기록된다.

  - cluster 노드 ip 정보 조회

    ```bash
    $ kubectl get ciliumnodes 
    NAME         CILIUMINTERNALIP   INTERNALIP       AGE
    cilium-ctr   172.20.2.28        192.168.10.100   24h
    cilium-w1    172.20.0.232       192.168.10.101   24h
    cilium-w2    172.20.1.18        192.168.10.102   24h
    ```

  - `curl-pod` 정보 조회

    ```bash
    $ kubectl get pod -owide
    NAME                      READY   STATUS    RESTARTS        AGE    IP             NODE         NOMINATED NODE   READINESS GATES
    curl-pod                  1/1     Running   1 (7h45m ago)   17h    172.20.2.80    cilium-ctr   <none>           <none>
    webpod-697b545f57-g9c5f   1/1     Running   0               17h    172.20.1.38    cilium-w2    <none>           <none>
    webpod-697b545f57-qgl6m   1/1     Running   0               148m   172.20.1.220   cilium-w2    <none>           <none>
    ```

  - `cilium-ctr`노드와 `route-vm` 노드의 ip, route 설정 정보 조회

    ```bash
    $ for i in ctr r ; do echo ">> node : cilium-$i <<"; sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@cilium-$i ip -c -br addr show | grep eth1; echo; done
    >> node : cilium-ctr <<
    eth1             UP             192.168.10.100/24 fe80::a00:27ff:fe30:52ea/64 

    >> node : cilium-r <<
    eth1             UP             192.168.10.200/24 fe80::a00:27ff:fe7b:379e/64 
    ```

    ```bash
    $ for i in ctr r ; do echo ">> node : cilium-$i <<"; sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@cilium-$i ip -c -br route show; echo; done
    >> node : cilium-ctr <<
    default via 10.0.2.2 dev eth0 proto dhcp src 10.0.2.15 metric 100 
    10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100 
    10.0.2.2 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
    10.0.2.3 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
    172.20.0.0/24 via 192.168.10.101 dev eth1 proto kernel 
    172.20.1.0/24 via 192.168.10.102 dev eth1 proto kernel 
    172.20.2.5 dev lxc_health proto kernel scope link 
    172.20.2.244 dev lxc49adfa975abf proto kernel scope link 
    192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.100 

    >> node : cilium-r <<
    default via 10.0.2.2 dev eth0 proto dhcp src 10.0.2.15 metric 100 
    10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100 
    10.0.2.2 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
    10.0.2.3 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
    192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.200 
    ```

  - `cilium-ctr`노드와 `route-vm` 통신 테스트

    ```bash
    $ ping 192.168.10.200
    PING 192.168.10.200 (192.168.10.200) 56(84) bytes of data.
    64 bytes from 192.168.10.200: icmp_seq=1 ttl=64 time=0.534 ms
    64 bytes from 192.168.10.200: icmp_seq=2 ttl=64 time=0.302 ms
    ```

    ```bash
    # route-vm tcpdump
    $ tcpdump -i eth1 icmp
    tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
    listening on eth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    17:25:16.014947 IP 192.168.10.100 > cilium-r: ICMP echo request, id 38246, seq 1, length 64
    17:25:16.014998 IP cilium-r > 192.168.10.100: ICMP echo reply, id 38246, seq 1, length 64
    17:25:17.050316 IP 192.168.10.100 > cilium-r: ICMP echo request, id 38246, seq 2, length 64
    17:25:17.050343 IP cilium-r > 192.168.10.100: ICMP echo reply, id 38246, seq 2, length 64
    ```

  - `curl-pod`에서 `cilium-ctr` 통신 테스트    

    ```bash
    $ kubectl exec -it curl-pod -- ping 192.168.10.100
    PING 192.168.10.100 (192.168.10.100) 56(84) bytes of data.
    64 bytes from 192.168.10.100: icmp_seq=1 ttl=64 time=0.052 ms
    64 bytes from 192.168.10.100: icmp_seq=2 ttl=64 time=0.052 ms
    ```

    ```bash
    # curl-pod tcpdump
    $ kubectl exec -it curl-pod -- tcpdump -i eth0 icmp
    tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
    listening on eth0, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    08:38:05.178133 IP curl-pod > 192.168.10.100: ICMP echo request, id 25, seq 1, length 64
    08:38:05.178186 IP 192.168.10.100 > curl-pod: ICMP echo reply, id 25, seq 1, length 64
    08:38:06.216058 IP curl-pod > 192.168.10.100: ICMP echo request, id 25, seq 2, length 64
    08:38:06.216096 IP 192.168.10.100 > curl-pod: ICMP echo reply, id 25, seq 2, length 64
    ```

  - `curl-pod`에서 `route-vm` 통신 테스트

    ```bash
    $ kubectl exec -it curl-pod -- ping 192.168.10.200
    PING 192.168.10.200 (192.168.10.200) 56(84) bytes of data.
    64 bytes from 192.168.10.200: icmp_seq=1 ttl=63 time=0.288 ms
    64 bytes from 192.168.10.200: icmp_seq=2 ttl=63 time=0.289 ms
    ```

    ```bash
    # curl-pod tcpdump
    $ kubectl exec -it curl-pod -- tcpdump -i eth0 icmp
    tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
    listening on eth0, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    08:41:05.853488 IP curl-pod > 192.168.10.200: ICMP echo request, id 48, seq 1, length 64
    08:41:05.853771 IP 192.168.10.200 > curl-pod: ICMP echo reply, id 48, seq 1, length 64
    08:41:06.888037 IP curl-pod > 192.168.10.200: ICMP echo request, id 48, seq 2, length 64
    08:41:06.888305 IP 192.168.10.200 > curl-pod: ICMP echo reply, id 48, seq 2, length 64

    # cilium-ctr tcpdump
    $ tcpdump -i eth1 icmp
    tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
    listening on eth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    17:41:05.853517 IP cilium-ctr > 192.168.10.200: ICMP echo request, id 42759, seq 1, length 64
    17:41:05.853771 IP 192.168.10.200 > cilium-ctr: ICMP echo reply, id 42759, seq 1, length 64
    17:41:06.888056 IP cilium-ctr > 192.168.10.200: ICMP echo request, id 42759, seq 2, length 64
    17:41:06.888305 IP 192.168.10.200 > cilium-ctr: ICMP echo reply, id 42759, seq 2, length 64
    
    # cilium-r tcpdump
    $ tcpdump -i eth1 icmp
    tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
    listening on eth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    17:41:04.073076 IP 192.168.10.100 > cilium-r: ICMP echo request, id 42759, seq 1, length 64   # curl-pod의 172.20.2.80 가 아닌 cilium-ctr의 192.168.10.100 이 찍힌다.
    17:41:04.073113 IP cilium-r > 192.168.10.100: ICMP echo reply, id 42759, seq 1, length 64
    17:41:05.108132 IP 192.168.10.100 > cilium-r: ICMP echo request, id 42759, seq 2, length 64
    17:41:05.108166 IP cilium-r > 192.168.10.100: ICMP echo reply, id 42759, seq 2, length 64
    ```

<br>

#### 2.2 Cluster 외부의 다른 네트워크 대역(10.0.0.0/16)에 있는 네트워크와의 통신 테스트
SS
- router VM에 Dummy Interface(loop1, loop2)를 만들어 두고, 해당 대역과의 통신을 테스트 한다.

  ```bash
  $ ip -c -br addr show
  lo               UNKNOWN        127.0.0.1/8 ::1/128 
  eth0             UP             10.0.2.15/24 metric 100 fd17:625c:f037:2:a00:27ff:fe6b:69c9/64 fe80::a00:27ff:fe6b:69c9/64 
  eth1             UP             192.168.10.200/24 fe80::a00:27ff:fe7b:379e/64 
  loop1            UNKNOWN        10.10.1.200/24 fe80::c837:49ff:fe98:2de4/64 
  loop2            UNKNOWN        10.10.2.200/24 fe80::fc26:82ff:fec4:214/64 
  ```

- Pod 에서는 Masquerading 과정을 거쳐 노드의 129.168.10.0 대역의 IP로 NAT 되어 통신되기 때문에 별도의 라우팅 경로 설정 없이 통신이 가능하다.

  ```bash
  $ ip -c -br route show
  default via 10.0.2.2 dev eth0 proto dhcp src 10.0.2.15 metric 100 
  10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100 
  10.0.2.2 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
  10.0.2.3 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
  10.10.1.0/24 dev loop1 proto kernel scope link src 10.10.1.200 
  10.10.2.0/24 dev loop2 proto kernel scope link src 10.10.2.200 
  192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.200
  ```

- `cilium-ctr`에서 `route-vm` Dummy Interface(loop1)와 통신 테스트

  ```bash
  ping 10.10.1.200
  PING 10.10.1.200 (10.10.1.200) 56(84) bytes of data.
  64 bytes from 10.10.1.200: icmp_seq=1 ttl=64 time=0.309 ms
  64 bytes from 10.10.1.200: icmp_seq=2 ttl=64 time=0.294 ms
  ```

- `curl-pod`에서 `route-vm`의 Dummy Interface(loop1)와 통신 테스트

  ```bash
  $ kubectl exec -it curl-pod -- curl -s 10.10.1.200
  <h1>Web Server : router</h1>
  ```

  ```bash
  # curl-pod tcpdump
  $ k exec -it curl-pod -- tcpdump -i eth0 tcp port 80
  tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
  listening on eth0, link-type EN10MB (Ethernet), snapshot length 262144 bytes
  22:08:00.208734 IP curl-pod.52094 > 10.10.1.200.80: Flags [S], seq 237902051, win 64240, options [mss 1460,sackOK,TS val 633285940 ecr 0,nop,wscale 7], length 0
  22:08:00.209027 IP 10.10.1.200.80 > curl-pod.52094: Flags [S.], seq 674184139, ack 237902052, win 65160, options [mss 1460,sackOK,TS val 3608092320 ecr 633285940,nop,wscale 7], length 0
  22:08:00.209062 IP curl-pod.52094 > 10.10.1.200.80: Flags [.], ack 1, win 502, options [nop,nop,TS val 633285941 ecr 3608092320], length 0
  22:08:00.209193 IP curl-pod.52094 > 10.10.1.200.80: Flags [P.], seq 1:76, ack 1, win 502, options [nop,nop,TS val 633285941 ecr 3608092320], length 75: HTTP: GET / HTTP/1.1
  22:08:00.209335 IP 10.10.1.200.80 > curl-pod.52094: Flags [.], ack 76, win 509, options [nop,nop,TS val 3608092320 ecr 633285941], length 0
  22:08:00.209758 IP 10.10.1.200.80 > curl-pod.52094: Flags [P.], seq 1:257, ack 76, win 509, options [nop,nop,TS val 3608092320 ecr 633285941], length 256: HTTP: HTTP/1.1 200 OK
  22:08:00.209782 IP curl-pod.52094 > 10.10.1.200.80: Flags [.], ack 257, win 501, options [nop,nop,TS val 633285941 ecr 3608092320], length 0
  22:08:00.209925 IP curl-pod.52094 > 10.10.1.200.80: Flags [F.], seq 76, ack 257, win 501, options [nop,nop,TS val 633285941 ecr 3608092320], length 0
  22:08:00.210141 IP 10.10.1.200.80 > curl-pod.52094: Flags [F.], seq 257, ack 77, win 509, options [nop,nop,TS val 3608092321 ecr 633285941], length 0
  22:08:00.210162 IP curl-pod.52094 > 10.10.1.200.80: Flags [.], ack 258, win 501, options [nop,nop,TS val 633285942 ecr 3608092321], length 0
  # cilium-ctr tcpdump
  $ tcpdump -i eth1 tcp port 80
  tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
  listening on eth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
  07:08:00.208767 IP cilium-ctr.52094 > 10.10.1.200.http: Flags [S], seq 237902051, win 64240, options [mss 1460,sackOK,TS val 633285940 ecr 0,nop,wscale 7], length 0
  07:08:00.209027 IP 10.10.1.200.http > cilium-ctr.52094: Flags [S.], seq 674184139, ack 237902052, win 65160, options [mss 1460,sackOK,TS val 3608092320 ecr 633285940,nop,wscale 7], length 0
  07:08:00.209080 IP cilium-ctr.52094 > 10.10.1.200.http: Flags [.], ack 1, win 502, options [nop,nop,TS val 633285941 ecr 3608092320], length 0
  07:08:00.209211 IP cilium-ctr.52094 > 10.10.1.200.http: Flags [P.], seq 1:76, ack 1, win 502, options [nop,nop,TS val 633285941 ecr 3608092320], length 75: HTTP: GET / HTTP/1.1
  07:08:00.209335 IP 10.10.1.200.http > cilium-ctr.52094: Flags [.], ack 76, win 509, options [nop,nop,TS val 3608092320 ecr 633285941], length 0
  07:08:00.209758 IP 10.10.1.200.http > cilium-ctr.52094: Flags [P.], seq 1:257, ack 76, win 509, options [nop,nop,TS val 3608092320 ecr 633285941], length 256: HTTP: HTTP/1.1 200 OK
  07:08:00.209802 IP cilium-ctr.52094 > 10.10.1.200.http: Flags [.], ack 257, win 501, options [nop,nop,TS val 633285941 ecr 3608092320], length 0
  07:08:00.209967 IP cilium-ctr.52094 > 10.10.1.200.http: Flags [F.], seq 76, ack 257, win 501, options [nop,nop,TS val 633285941 ecr 3608092320], length 0
  07:08:00.210141 IP 10.10.1.200.http > cilium-ctr.52094: Flags [F.], seq 257, ack 77, win 509, options [nop,nop,TS val 3608092321 ecr 633285941], length 0
  07:08:00.210184 IP cilium-ctr.52094 > 10.10.1.200.http: Flags [.], ack 258, win 501, options [nop,nop,TS val 633285942 ecr 3608092321], length 0
  # cilium-r tcpdump
  $ tcpdump -i eth1 tcp port 80
  tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
  listening on eth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
  07:08:00.340767 IP 192.168.10.100.52094 > cilium-r.http: Flags [S], seq 237902051, win 64240, options [mss 1460,sackOK,TS val 633285940 ecr 0,nop,wscale 7], length 0
  07:08:00.340802 IP cilium-r.http > 192.168.10.100.52094: Flags [S.], seq 674184139, ack 237902052, win 65160, options [mss 1460,sackOK,TS val 3608092320 ecr 633285940,nop,wscale 7], length 0
  07:08:00.341009 IP 192.168.10.100.52094 > cilium-r.http: Flags [.], ack 1, win 502, options [nop,nop,TS val 633285941 ecr 3608092320], length 0
  07:08:00.341134 IP 192.168.10.100.52094 > cilium-r.http: Flags [P.], seq 1:76, ack 1, win 502, options [nop,nop,TS val 633285941 ecr 3608092320], length 75: HTTP: GET / HTTP/1.1
  07:08:00.341159 IP cilium-r.http > 192.168.10.100.52094: Flags [.], ack 76, win 509, options [nop,nop,TS val 3608092320 ecr 633285941], length 0
  07:08:00.341565 IP cilium-r.http > 192.168.10.100.52094: Flags [P.], seq 1:257, ack 76, win 509, options [nop,nop,TS val 3608092320 ecr 633285941], length 256: HTTP: HTTP/1.1 200 OK
  07:08:00.341720 IP 192.168.10.100.52094 > cilium-r.http: Flags [.], ack 257, win 501, options [nop,nop,TS val 633285941 ecr 3608092320], length 0
  07:08:00.341894 IP 192.168.10.100.52094 > cilium-r.http: Flags [F.], seq 76, ack 257, win 501, options [nop,nop,TS val 633285941 ecr 3608092320], length 0
  07:08:00.341960 IP cilium-r.http > 192.168.10.100.52094: Flags [F.], seq 257, ack 77, win 509, options [nop,nop,TS val 3608092321 ecr 633285941], length 0
  07:08:00.342107 IP 192.168.10.100.52094 > cilium-r.http: Flags [.], ack 258, win 501, options [nop,nop,TS val 633285942 ecr 3608092321], length 0
  ```

#### 2.3 Cluster 외부의 다른 네트워크 대역(10.0.0.0/16)과 Masquerade 없이 통신하도록 설정하는 방법

- Cluster 외부의 다른 네트워크와 Pod의 IP 대역으로 NAT 없이 통신 하기 위해서는 `ip-masq-agent`를 활성화 해야 한다.

  - helm을 통해 cilium을 생성할 때 `ipMasqAgent.enabled=true` 파라미터를 추가해야 한다.

  - `ipMasqAgent` 파라미터와 연동해서 Masquerade 없이 통신할 Target CIDR 값을 `ipMasqAgent.config.nonMasqueradeCIDRs` 파라미터로 설정할 수 있다.

    ```bash
    helm upgrade cilium cilium/cilium --namespace kube-system --reuse-values \
    --set ipMasqAgent.enabled=true --set ipMasqAgent.config.nonMasqueradeCIDRs='{10.10.1.0/24,10.10.2.0/24}'
    ```

  - 설정 결과 확인 및 추가 설정 정보 수정은 ConfigMap `ip-masq-agent`을 이용한다.

    ```bash
    $ kubectl get cm ip-masq-agent -n kube-system -oyaml
    apiVersion: v1
    data:
      config: '{"nonMasqueradeCIDRs":["10.10.1.0/24","10.10.2.0/24"]}'
    kind: ConfigMap
    metadata:
      annotations:
        meta.helm.sh/release-name: cilium
        meta.helm.sh/release-namespace: kube-system
      creationTimestamp: "2025-07-30T02:26:52Z"
      labels:
        app.kubernetes.io/managed-by: Helm
      name: ip-masq-agent
      namespace: kube-system
      resourceVersion: "311691"
      uid: edcd486d-97a2-493f-b1db-0574bd47315f
      ```

- Cluster 내부 리소스와 통신하려는 사내망의 네트워크에서도 Cluster 내부 리소스와 통신할 경로 값이 설정되어 있어야 한다.

  - `ipMasqAgent` 설정 후 라우팅 정보를 추가 하지 않은 상태에서 동일하게 `curl-pod`에서 HTTP 트래픽을 사용하면 응답이 오지 않는다.

    ```bash
    $ kubectl exec -it curl-pod -- curl -s 10.10.1.200
    ```

    ```bash
    # cilium-ctr tcpdump
    $ tcpdump -i eth1 tcp port 80
    tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
    listening on eth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    11:40:46.764927 IP 172.20.2.80.40900 > 10.10.1.200.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649652496 ecr 0,nop,wscale 7], length 0
    11:40:47.816066 IP 172.20.2.80.40900 > 10.10.1.200.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649653548 ecr 0,nop,wscale 7], length 0
    11:40:48.840032 IP 172.20.2.80.40900 > 10.10.1.200.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649654572 ecr 0,nop,wscale 7], length 0
    11:40:49.864038 IP 172.20.2.80.40900 > 10.10.1.200.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649655596 ecr 0,nop,wscale 7], length 0
    11:40:50.888112 IP 172.20.2.80.40900 > 10.10.1.200.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649656620 ecr 0,nop,wscale 7], length 0
    11:40:51.912075 IP 172.20.2.80.40900 > 10.10.1.200.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649657644 ecr 0,nop,wscale 7], length 0
    11:40:53.960031 IP 172.20.2.80.40900 > 10.10.1.200.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649659692 ecr 0,nop,wscale 7], length 0
    11:40:57.992070 IP 172.20.2.80.40900 > 10.10.1.200.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649663724 ecr 0,nop,wscale 7], length 0
    11:41:06.056072 IP 172.20.2.80.40900 > 10.10.1.200.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649671788 ecr 0,nop,wscale 7], length 0
    11:41:22.440062 IP 172.20.2.80.40900 > 10.10.1.200.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649688172 ecr 0,nop,wscale 7], length 0
    # cilium-r tcpdump
    $ tcpdump -i eth1 tcp port 80
    tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
    listening on eth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    11:40:46.770814 IP 172.20.2.80.40900 > cilium-r.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649652496 ecr 0,nop,wscale 7], length 0
    11:40:47.821961 IP 172.20.2.80.40900 > cilium-r.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649653548 ecr 0,nop,wscale 7], length 0
    11:40:48.845932 IP 172.20.2.80.40900 > cilium-r.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649654572 ecr 0,nop,wscale 7], length 0
    11:40:49.869943 IP 172.20.2.80.40900 > cilium-r.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649655596 ecr 0,nop,wscale 7], length 0
    11:40:50.893936 IP 172.20.2.80.40900 > cilium-r.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649656620 ecr 0,nop,wscale 7], length 0
    11:40:51.917980 IP 172.20.2.80.40900 > cilium-r.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649657644 ecr 0,nop,wscale 7], length 0
    11:40:53.965934 IP 172.20.2.80.40900 > cilium-r.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649659692 ecr 0,nop,wscale 7], length 0
    11:40:57.997976 IP 172.20.2.80.40900 > cilium-r.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649663724 ecr 0,nop,wscale 7], length 0
    11:41:06.062041 IP 172.20.2.80.40900 > cilium-r.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649671788 ecr 0,nop,wscale 7], length 0
    11:41:22.445981 IP 172.20.2.80.40900 > cilium-r.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649688172 ecr 0,nop,wscale 7], length 0
    11:41:54.702038 IP 172.20.2.80.40900 > cilium-r.http: Flags [S], seq 53382965, win 64240, options [mss 1460,sackOK,TS val 649720428 ecr 0,nop,wscale 7], length 0
    ```

  - `route-vm`의 tcpdump 결과 172.20.2.80(curl-pod) IP 값이 조회되는데, route-vm의 라우팅 경로에는 172.20.0.0/24 CIDR 대역에 대한 경로 정보가 없다.

    ```bash
    $ ip -c -br route show
    default via 10.0.2.2 dev eth0 proto dhcp src 10.0.2.15 metric 100 
    10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100 
    10.0.2.2 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
    10.0.2.3 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
    10.10.1.0/24 dev loop1 proto kernel scope link src 10.10.1.200 
    10.10.2.0/24 dev loop2 proto kernel scope link src 10.10.2.200 
    192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.200
    ```

  - 각 노드에는 Pod에 할당할 PodCIDR이 정해져 있다. 각 PodCIDR을 해당 노드의 인터페이스로 향하는 경로를 설정해야 한다.

    ```bash
    $ kubectl get ciliumnode -o jsonpath='{range .items[*]}{.metadata.name}{" "}{.spec.ipam.podCIDRs[*]}{"\n"}{end}' | column -t
    cilium-ctr  172.20.2.0/24
    cilium-w1   172.20.0.0/24
    cilium-w2   172.20.1.0/24
    ```

    ```bash
    kubectl get ciliumnode
    NAME         CILIUMINTERNALIP   INTERNALIP       AGE
    cilium-ctr   172.20.2.28        192.168.10.100   43h
    cilium-w1    172.20.0.232       192.168.10.101   43h
    cilium-w2    172.20.1.18        192.168.10.102   43h
    ```

    ```bash
    # cilium-r 노드에서 진행
    ip route add 172.20.2.0/24 via 192.168.10.100
    ip route add 172.20.0.0/24 via 192.168.10.101
    ip route add 172.20.1.0/24 via 192.168.10.102
    ```

  - `route-vm` 라우팅 경로 추가 정보 확인 

    ```bash
    ip -c -br route show
    default via 10.0.2.2 dev eth0 proto dhcp src 10.0.2.15 metric 100 
    10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100 
    10.0.2.2 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
    10.0.2.3 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100 
    10.10.1.0/24 dev loop1 proto kernel scope link src 10.10.1.200 
    10.10.2.0/24 dev loop2 proto kernel scope link src 10.10.2.200 
    172.20.0.0/24 via 192.168.10.101 dev eth1 
    172.20.1.0/24 via 192.168.10.102 dev eth1 
    172.20.2.0/24 via 192.168.10.100 dev eth1 
    192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.200
    ```

  - 라우팅 경로 추가 후 다시 통신 테스트 결과 이번에는 응답이 오는 것을 확인할 수 있다.
  
    ```bash
    $ kubectl exec -it curl-pod -- curl -s 10.10.1.200
    <h1>Web Server : router</h1>
    ```

    ```bash
    # cilium-ctr tcpdump 
    $ tcpdump -i eth1 tcp port 80
    tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
    listening on eth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    11:52:53.344844 IP 172.20.2.80.34686 > 10.10.1.200.http: Flags [S], seq 509259083, win 64240, options [mss 1460,sackOK,TS val 650379076 ecr 0,nop,wscale 7], length 0
    11:52:53.345103 IP 10.10.1.200.http > 172.20.2.80.34686: Flags [S.], seq 4247483222, ack 509259084, win 65160, options [mss 1460,sackOK,TS val 4031452192 ecr 650379076,nop,wscale 7], length 0
    11:52:53.345177 IP 172.20.2.80.34686 > 10.10.1.200.http: Flags [.], ack 1, win 502, options [nop,nop,TS val 650379077 ecr 4031452192], length 0
    11:52:53.345341 IP 172.20.2.80.34686 > 10.10.1.200.http: Flags [P.], seq 1:76, ack 1, win 502, options [nop,nop,TS val 650379077 ecr 4031452192], length 75: HTTP: GET / HTTP/1.1
    11:52:53.345454 IP 10.10.1.200.http > 172.20.2.80.34686: Flags [.], ack 76, win 509, options [nop,nop,TS val 4031452192 ecr 650379077], length 0
    11:52:53.345757 IP 10.10.1.200.http > 172.20.2.80.34686: Flags [P.], seq 1:257, ack 76, win 509, options [nop,nop,TS val 4031452192 ecr 650379077], length 256: HTTP: HTTP/1.1 200 OK
    11:52:53.345793 IP 172.20.2.80.34686 > 10.10.1.200.http: Flags [.], ack 257, win 501, options [nop,nop,TS val 650379077 ecr 4031452192], length 0
    11:52:53.346009 IP 172.20.2.80.34686 > 10.10.1.200.http: Flags [F.], seq 76, ack 257, win 501, options [nop,nop,TS val 650379078 ecr 4031452192], length 0
    11:52:53.346181 IP 10.10.1.200.http > 172.20.2.80.34686: Flags [F.], seq 257, ack 77, win 509, options [nop,nop,TS val 4031452193 ecr 650379078], length 0
    11:52:53.346227 IP 172.20.2.80.34686 > 10.10.1.200.http: Flags [.], ack 258, win 501, options [nop,nop,TS val 650379078 ecr 4031452193], length 0
    # cilium-r tcpdump
    $ tcpdump -i eth1 tcp port 80
    tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
    listening on eth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
    11:52:53.350732 IP 172.20.2.80.34686 > cilium-r.http: Flags [S], seq 509259083, win 64240, options [mss 1460,sackOK,TS val 650379076 ecr 0,nop,wscale 7], length 0
    11:52:53.350774 IP cilium-r.http > 172.20.2.80.34686: Flags [S.], seq 4247483222, ack 509259084, win 65160, options [mss 1460,sackOK,TS val 4031452192 ecr 650379076,nop,wscale 7], length 0
    11:52:53.350964 IP 172.20.2.80.34686 > cilium-r.http: Flags [.], ack 1, win 502, options [nop,nop,TS val 650379077 ecr 4031452192], length 0
    11:52:53.351125 IP 172.20.2.80.34686 > cilium-r.http: Flags [P.], seq 1:76, ack 1, win 502, options [nop,nop,TS val 650379077 ecr 4031452192], length 75: HTTP: GET / HTTP/1.1
    11:52:53.351146 IP cilium-r.http > 172.20.2.80.34686: Flags [.], ack 76, win 509, options [nop,nop,TS val 4031452192 ecr 650379077], length 0
    11:52:53.351427 IP cilium-r.http > 172.20.2.80.34686: Flags [P.], seq 1:257, ack 76, win 509, options [nop,nop,TS val 4031452192 ecr 650379077], length 256: HTTP: HTTP/1.1 200 OK
    11:52:53.351586 IP 172.20.2.80.34686 > cilium-r.http: Flags [.], ack 257, win 501, options [nop,nop,TS val 650379077 ecr 4031452192], length 0
    11:52:53.351797 IP 172.20.2.80.34686 > cilium-r.http: Flags [F.], seq 76, ack 257, win 501, options [nop,nop,TS val 650379078 ecr 4031452192], length 0
    11:52:53.351856 IP cilium-r.http > 172.20.2.80.34686: Flags [F.], seq 257, ack 77, win 509, options [nop,nop,TS val 4031452193 ecr 650379078], length 0
    11:52:53.352005 IP 172.20.2.80.34686 > cilium-r.http: Flags [.], ack 258, win 501, options [nop,nop,TS val 650379078 ecr 4031452193], length 0
    ```

  - 소규모 작업 환경에서는 관리자가 수동으로 라우팅 설정하는 것이 가능하지만 대규모 환경에서는 BGP를 이용해 동적으로 라우팅 정보를 전파하는 것이 좋다.

<br>

### 3. 네트워크 엔드포인트(컨테이너/LB) IP 관리를 위한 IPAM (IP Address Management)

<p align=center><img src="./_image/ipam_mode.png" title="출처: Kubernetes Networking & Cilium for Network Engineers - An Instruction Manual" width="60%"></p>

- 네트워크 엔드포인트(컨테이너 등)에서 사용할 IP 주소를 할당하고 관리하는 역할을 한다.

- IPAM을 한 번 설정한 상태에서 모드를 변경하는 것은 권장되지 않는다. 새 IPAM 구성으로 새로운 쿠버네티스 클러스터를 생성하는 것이 좋다.

- 배포 환경, 사용자의 요구사항에 따라 다양하게 구성이 가능하도록 7개의 배포 모드를 지원한다. [ [docs](https://docs.cilium.io/en/stable/network/concepts/ipam/) ]

<br>

#### 3.1 Kubernetes Host Scope

<p align=center><img src="./_image/kubernetest_host_scope_ipam.png" title="출처: ISOVALENT_BLOG" width="60%"></p>

- `Kubernetes Controller Manager`가 Node에 할당한 PodCIDR을 사용한다.

- 이 모드는 간단하게 구현이 가능하지만 PodCIDR을 유연하게 관리하는 것은 제약이 많다.

  - 클러스터 전체에 하나의 PodCIDR 구성만 지원한다.

  - 클러스터의 PodCIDR이 고갈되었을 때 클러스터나 개별 노드에 PodCIDR을 추가하는 것이 불가능하다.

  - 초기 클러스터 배포 시 정확하게 노드별 IP 주소 계획을 수립해서 사용해야 한다.

<br>

#### 3.2 Cluster Scope IPAM (Default)

<p align=center><img src="./_image/cluster_scope_ipam.png" title="출처: ISOVALENT_BLOG" width="50%"></p>

- Kubernetes Host Scope와 동일하게 Node에 할당된 PodCIDR을 활용해 IP를 할당한다.

- Node에 할당된 PodCIDR을 할당하는 주체가 `Kubernetes Controller Manager`가 아닌 `Cilium Operator`가 된다.

- Cluster Scope IPAM의 장점은 여러 CIDR을 할당할 수 있다는 것이다.

- 하지만, 할당된 IP Pool의 주소 고갈 문제를 완전히 해결하지는 못하는 문제는 Kubernetes Host Scope와 동일하다.

##### Cluster Scope IPAM 모드에서 CIDR 추가 테스트

- cluster scope ipam cidr 설정 파일을 생성한다.

  ```yaml
  # cilium-values.yaml
  ---
  ipam:
    mode: cluster-pool
    operator:
      clusterPoolIPv4MaskSize: 29
      clusterPoolIPv4PodCIDRList:
        - "172.20.42.0/28"
        - "172.20.84.0/28"
  ```

- Helm 도구를 이용해서 1.17.5 Cilium을 설치하면서 설정파일을 `-f` 옵션으로 할당한다.

  ```bash
  helm repo add cilium https://helm.cilium.io/
  helm repo update
  helm install cilium cilium/cilium -n kube-system --version 1.17.5 -f cilium-values.yaml
  ```

- Cilium CLI를 이용해 배포 결과를 확인한다. (CLI는 별도 설치 필요)

  ```bash
  $ cilium config view | grep cluster-pool
  cluster-pool-ipv4-cidr                            172.20.42.0/28 172.20.84.0/28
  cluster-pool-ipv4-mask-size                       29
  ipam                                              cluster-pool
  ```

- `172.20.42.0/28`, `172.20.84.0/28` CIDR 이 외에 추가 CIDR을 파일에 반영 후 Helm을 업데이트한다.

  ```yaml
  # cilium-values.yaml
  ---
  ipam:
    mode: cluster-pool
    operator:
      clusterPoolIPv4MaskSize: 29
      clusterPoolIPv4PodCIDRList:
        - "172.20.42.0/28"
        - "172.20.84.0/28"
        - "10.0.0.0/28"
  ```

  ```bash
  helm upgrade cilium cilium/cilium -n kube-system --version 1.17.5 -f cilium-values.yaml
  ```

- Cilium CLI를 이용해 배포 결과를 확인한다.

  ```bash
  $ cilium config view | grep cluster-pool
  cluster-pool-ipv4-cidr                            172.20.42.0/28 172.20.84.0/28 10.0.0.0/28
  cluster-pool-ipv4-mask-size                       29
  ipam                                              cluster-pool
  ```

- node에 할당된 cidr 정보를 확인한다.

  ```bash
  $ kubectl get ciliumnode -o jsonpath='{range .items[*]}{.metadata.name}{" "}{.spec.ipam.podCIDRs[*]}{"\n"}{end}' | column -t
  cilium-ctr  172.20.84.0/29
  cilium-w1   172.20.42.0/29
  cilium-w2   172.20.42.8/29
  ```

##### IP Pool이 고갈된 상황 테스트

- `cilium-w1`, `cilium-w2` 노드에 할당된 CIDR Pool 정보와 남은 IP의 개수를 확인한다.

  ```bash
  $ c1dbg status | grep IPAM
  IPAM:                    IPv4: 3/6 allocated from 172.20.42.0/29,
  $ c2dbg status | grep IPAM
  IPAM:                    IPv4: 3/6 allocated from 172.20.42.8/29,
  ```

- 노드에 할당된 IP의 정보를 세부적으로 확인하기 위해서 `--verbose` 옵션을 추가하면 router, health, coredns 정보를 확인할 수 있다.

  ```bash
  $ c1dbg status --verbose | grep -A6 "Allocated"
  Allocated addresses:
    172.20.42.2 (router)
    172.20.42.3 (health)
    172.20.42.5 (kube-system/coredns-674b8bbfcf-xcvrr)
  ```

- 호스트 노드에 할당된 인터페이스의 정보를 조회하면 router에 할당된 IP와 같은 것을 확인할 수 있다.

  ```bash
  $ sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@cilium-w1 ip -c -br addr show
  ...
  cilium_host@cilium_net  UP             172.20.42.2/32 fe80::4cb3:63ff:fe9a:4b8b/64
  lxc_health@if6          UP             fe80::c07d:afff:fe65:d624/64
  lxc31994ffe5fef@if8     UP             fe80::5c67:79ff:fec9:4eb3/64
  ```

- coredns에 할당된 IP 정보를 `kubectl` 명령으로 확인하면 해당 정보도 cilium agent에서 확인한 정보와 같은 것을 볼 수 있다.

  ```bash
  $ k get po -A -owide | grep coredns | grep cilium-w1
  kube-system   coredns-674b8bbfcf-xcvrr             1/1     Running   1 (22m ago)   34h    172.20.42.5      cilium-w1    <none>           <none>
  ```

- 각 노드에 3개의 IP 가용량만 남은 상태에서 IP를 고갈 시 동작 결과를 확인하기 위해 10개의 Pod를 생성한다.

  ```bash
  $ kubectl create deployment netshoot --image nginx --replicas 10
  deployment.apps/netshoot created
  ```

- 생성 후 결과를 조회하면 4개의 Pod가 `ContainerCreating` 단계에서 넘어가지 못하고 홀딩 상태 유지하는 것을 볼 수 있는데,

  ```bash
  $ kubectl get po -o wide
  NAME                        READY   STATUS              RESTARTS   AGE     IP             NODE        NOMINATED NODE   READINESS GATES
  netshoot-5fbc6f4997-99djz   1/1     Running             0          2m34s   172.20.42.9    cilium-w2   <none>           <none>
  netshoot-5fbc6f4997-9h945   1/1     Running             0          2m34s   172.20.42.10   cilium-w2   <none>           <none>
  netshoot-5fbc6f4997-9h9vw   0/1     ContainerCreating   0          2m34s   <none>         cilium-w2   <none>           <none>
  netshoot-5fbc6f4997-b9mdk   0/1     ContainerCreating   0          2m34s   <none>         cilium-w2   <none>           <none>
  netshoot-5fbc6f4997-rdbg8   1/1     Running             0          2m34s   172.20.42.4    cilium-w1   <none>           <none>
  netshoot-5fbc6f4997-t84kf   0/1     ContainerCreating   0          2m34s   <none>         cilium-w1   <none>           <none>
  netshoot-5fbc6f4997-tq5tt   0/1     ContainerCreating   0          2m34s   <none>         cilium-w1   <none>           <none>
  netshoot-5fbc6f4997-wpf8j   1/1     Running             0          2m34s   172.20.42.1    cilium-w1   <none>           <none>
  netshoot-5fbc6f4997-xjft8   1/1     Running             0          2m34s   172.20.42.11   cilium-w2   <none>           <none>
  netshoot-5fbc6f4997-zxzfs   1/1     Running             0          2m34s   172.20.42.6    cilium-w1   <none>           <none>
  ```

- 배포 완료되지 못한 특정 파드의 이벤트 정보를 확인하면, kubelet이 Cilium Plugin에서 IP주소를 가져오지 못하고 "range is full" 메세지를 보여준다.

  ```bash
  $ kubectl describe po netshoot-5fbc6f4997-tq5tt  | grep -A10 Events
  Events:
    Type     Reason                  Age                  From               Message
    ----     ------                  ----                 ----               -------
    Normal   Scheduled               5m31s                default-scheduler  Successfully assigned default/netshoot-5fbc6f4997-tq5tt to cilium-w1
    Warning  FailedCreatePodSandBox  5m30s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "d0146b59d6be4c9e7fdd3e2d1fbba7b999d4c8bddf99f9248168bea00062d0d8": plugin type="cilium-cni" failed (add): unable to allocate IP via local cilium agent: [POST /ipam][502] postIpamFailure "range is full"
    ...
  ```

- 실제로 cilium agent의 디버깅 CLI로 남은 IP 수량을 확인하면 `6/6`으로 모든 IP가 소진되었고, 용 가능한 IP 주소가 없는 것을 확인할 수 있다.

  ```bash
  $ c1dbg status | grep IPAM
  IPAM:                    IPv4: 6/6 allocated from 172.20.42.0/29
  $ c2dbg status | grep IPAM
  IPAM:                    IPv4: 6/6 allocated from 172.20.42.8/29,
  ```

<br>

#### 3.3 Multi Pool

<p align=center><img src="./_image/multi_pool_ipam.png" title="출처: ISOVALENT_BLOG" width="60%"></p>

- Pod IP Pool을 여러 개 생성한 다음 같은 노드에서 생성되는 Pod에게 서로 다른 IP Pool을 할당할 수 있다.

- `CiliumPodIPPool`을 생성해서 Pod IP Pool을 노드에 동적으로 추가할 수 있다.

##### Pod CIDR 선택 배포 테스트

- multi-pool ipam cidr 설정 파일을 생성한다.

  ```yaml
  # cilium-values.yaml
  ---
  ipam:
    mode: multi-pool
    operator:
      autoCreateCiliumPodIPPools:
        default:
          ipv4:
            cidrs: 172.20.0.0/16
            maskSize: 28
  ```

  ```bash
  helm install cilium cilium/cilium -n kube-system --version 1.17.5 -f cilium-values.yaml
  ```

- Cilium CLI를 이용해 multi pool ipam 배포 결과를 확인한다.

  ```bash
  $ cilium config view | grep multi-pool
  ipam                                              multi-pool
  $ cilium config view | grep auto-create
  auto-create-cilium-pod-ip-pools                   default=ipv4-cidrs:172.20.0.0/16;ipv4-mask-size:28
  ```

- 각 노드에 할당된 cidr 정보를 확인한다.

  ```bash
  $ for i in ctr w1 w2 ; do echo ">> node : cilium-$i <<"; kubectl get ciliumnodes cilium-$i -o yaml | yq .spec.ipam.pools.allocated ; echo; done
  >> node : cilium-ctr <<
  [
    {
      "cidrs": [
        "172.20.0.0/28",
        "172.20.0.16/28"
      ],
      "pool": "default"
    }
  ]

  >> node : cilium-w1 <<
  [
    {
      "cidrs": [
        "172.20.0.48/28",
        "172.20.0.80/28"
      ],
      "pool": "default"
    }
  ]

  >> node : cilium-w2 <<
  [
    {
      "cidrs": [
        "172.20.0.32/28",
        "172.20.0.64/28"
      ],
      "pool": "default"
    }
  ]
  ```

- 기존에 cilium cni를 설치하는 과정에 설정된 cidr 이후에도 ip 대역을 추가할 때는 `CiliumPodIPPool`을 사용할 수 있다.

- `CiliumPodIPPool`를 이용해 vac, no-engine에서 사용할 IP Pool을 추가한다.

  ```yaml
  # cilium_ip_pool_vac.yaml
  apiVersion: cilium.io/v2alpha1
  kind: CiliumPodIPPool
  metadata:
    name: vac
  spec:
    ipv4:
      cidrs:
        - 10.100.0.0/16
      maskSize: 29
  # cilium_ip_pool_no.yaml
  apiVersion: cilium.io/v2alpha1
  kind: CiliumPodIPPool
  metadata:
    name: no-engine
  spec:
    ipv4:
      cidrs:
        - 10.200.0.0/16
      maskSize: 29
  ```

  ```bash
  kubectl apply -f cilium_ip_pool_vac.yaml
  kubectl apply -f cilium_ip_pool_no.yaml
  ```

- `CiliumPodIPPool`를 생성할 때 `metadata.name` 필드를 이용해서 vac, no-engine를 만들었는데,

- Pod의 매니페스트 파일에서 annotation을 지정하지 않으면 default ip pool을 사용하게 된다.

- default ip pool 할당 여부를 확인하기 위해 기본 설정으로 deployment를 생성해보고 할당되는 IP 주소가 `172.20.0.0/16`인지 확인한다.

  ```bash
  $ kubectl create deployment netshoot --image nginx --replicas 2
  $ kubectl get po -owide
  NAME                        READY   STATUS    RESTARTS   AGE   IP            NODE        NOMINATED NODE   READINESS GATES
  netshoot-5fbc6f4997-2dd4z   1/1     Running   0          92s   172.20.0.77   cilium-w2   <none>           <none>
  netshoot-5fbc6f4997-4tmch   1/1     Running   0          92s   172.20.0.89   cilium-w1   <none>           <none>
  ```

- 특정 ip pool을 사용하고자 할 때는 `spec.template.metadata.annotation` 필드에 사용하고자 하는 ip pool의 name을 지정한다.

  ```yaml
  # deploy_vac.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: nginx-vac
  spec:
    selector:
      matchLabels:
        app: nginx-vac
    replicas: 2
    template:
      metadata:
        labels:
          app: nginx-vac
        annotations:
          ipam.cilium.io/ip-pool: vac # ip pool name 지정
      spec:
        containers:
          - name: nginx
            image: nginx
            ports:
              - containerPort: 80
  ---
  # deploy_no.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: nginx-no
  spec:
    selector:
      matchLabels:
        app: nginx-no
    replicas: 2
    template:
      metadata:
        labels:
          app: nginx-no
        annotations:
          ipam.cilium.io/ip-pool: no-engine # ip pool name 지정
      spec:
        containers:
          - name: nginx
            image: nginx
            ports:
              - containerPort: 80
  ```

- 배포 후 vac Pod는 `10.100.0.0/16`, no Pod는 `10.200.0.0/16` 대역의 주소가 의도한 대로 할당되었는지 확인한다.

  ```bash
  $ kubectl get pods -o wide
  NAME                         READY   STATUS    RESTARTS   AGE     IP            NODE        NOMINATED NODE   READINESS GATES
  netshoot-5fbc6f4997-2dd4z    1/1     Running   0          6m19s   172.20.0.77   cilium-w2   <none>           <none>
  netshoot-5fbc6f4997-4tmch    1/1     Running   0          6m19s   172.20.0.89   cilium-w1   <none>           <none>
  nginx-no-76df566dfb-4kfpx    1/1     Running   0          20s     10.200.0.11   cilium-w1   <none>           <none>
  nginx-no-76df566dfb-d6v2j    1/1     Running   0          20s     10.200.0.3    cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-75rr6   1/1     Running   0          82s     10.100.0.14   cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-dsp56   1/1     Running   0          82s     10.100.0.6    cilium-w1   <none>           <none>
  ```

##### CIDR 풀 자동 확장 테스트

- CIDR IP Pool이 확장 되기 전에 현재 노드 마다 할당된 CIDR IP Pool 정보를 확인한다.

  ```bash
  $ for i in ctr w1 w2 ; do echo ">> node : cilium-$i <<"; kubectl get ciliumnodes cilium-$i -o yaml | yq .spec.ipam.pools.allocated ; echo; done
  >> node : cilium-ctr <<
  [
    {
      "cidrs": [
        "172.20.0.0/28",
        "172.20.0.16/28"
      ],
      "pool": "default"
    }
  ]

  >> node : cilium-w1 <<
  [
    {
      "cidrs": [
        "172.20.0.48/28",
        "172.20.0.80/28"
      ],
      "pool": "default"
    },
    {
      "cidrs": [
        "10.200.0.8/29"
      ],
      "pool": "no-engine"
    },
    {
      "cidrs": [
        "10.100.0.0/29"
      ],
      "pool": "vac"
    }
  ]

  >> node : cilium-w2 <<
  [
    {
      "cidrs": [
        "172.20.0.32/28",
        "172.20.0.64/28"
      ],
      "pool": "default"
    },
    {
      "cidrs": [
        "10.200.0.0/29"
      ],
      "pool": "no-engine"
    },
    {
      "cidrs": [
        "10.100.0.8/29"
      ],
      "pool": "vac"
    }
  ]
  ```

- 추가된 ip pool은 `/29`로 8개씩 IP를 할당할 수 있는데 고갈된 경우 동적으로 확장되는지 테스트 하기 위해 `nginx-vac` deployment의 replicas 개수를 20개로 수정한다.

  ```bash
  $ kubectl scale deployment nginx-vac --replicas=20
  ```

- Pod의 배포 결과를 확인해 보면 기존 할당된 ip pool 대비 더 많은 Pod가 정상적으로 배포 된 것을 볼 수 있다.

  ```bash
  $ kubectl get po -owide
  NAME                         READY   STATUS    RESTARTS   AGE     IP            NODE        NOMINATED NODE   READINESS GATES
  netshoot-5fbc6f4997-2dd4z    1/1     Running   0          14m     172.20.0.77   cilium-w2   <none>           <none>
  netshoot-5fbc6f4997-4tmch    1/1     Running   0          14m     172.20.0.89   cilium-w1   <none>           <none>
  nginx-no-76df566dfb-4kfpx    1/1     Running   0          8m30s   10.200.0.11   cilium-w1   <none>           <none>
  nginx-no-76df566dfb-d6v2j    1/1     Running   0          8m30s   10.200.0.3    cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-2fz58   1/1     Running   0          39s     10.100.0.28   cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-4h48r   1/1     Running   0          6m51s   10.100.0.1    cilium-w1   <none>           <none>
  nginx-vac-65889dbdc9-75rr6   1/1     Running   0          9m32s   10.100.0.14   cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-7q2zr   1/1     Running   0          39s     10.100.0.30   cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-96bcx   1/1     Running   0          6m51s   10.100.0.13   cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-9jfcp   1/1     Running   0          6m51s   10.100.0.11   cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-bdj98   1/1     Running   0          39s     10.100.0.22   cilium-w1   <none>           <none>
  nginx-vac-65889dbdc9-bttzt   1/1     Running   0          39s     10.100.0.20   cilium-w1   <none>           <none>
  nginx-vac-65889dbdc9-chbhz   1/1     Running   0          6m51s   10.100.0.10   cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-ck4q8   1/1     Running   0          39s     10.100.0.27   cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-cqgmd   1/1     Running   0          6m51s   10.100.0.2    cilium-w1   <none>           <none>
  nginx-vac-65889dbdc9-dsp56   1/1     Running   0          9m32s   10.100.0.6    cilium-w1   <none>           <none>
  nginx-vac-65889dbdc9-jlchn   1/1     Running   0          39s     10.100.0.9    cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-l2nz5   1/1     Running   0          39s     10.100.0.29   cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-ncsmb   1/1     Running   0          39s     10.100.0.21   cilium-w1   <none>           <none>
  nginx-vac-65889dbdc9-nr7zv   1/1     Running   0          6m51s   10.100.0.3    cilium-w1   <none>           <none>
  nginx-vac-65889dbdc9-rggpj   1/1     Running   0          6m51s   10.100.0.12   cilium-w2   <none>           <none>
  nginx-vac-65889dbdc9-s9s6w   1/1     Running   0          39s     10.100.0.4    cilium-w1   <none>           <none>
  nginx-vac-65889dbdc9-vn4bv   1/1     Running   0          39s     10.100.0.17   cilium-w1   <none>           <none>
  nginx-vac-65889dbdc9-xsz9h   1/1     Running   0          6m51s   10.100.0.5    cilium-w1   <none>           <none>
  ```

- 필요한 IP 수가 늘어나게 되면 CIDR IP Pool이 자동으로 확장 된다.

  ```bash
  $ for i in ctr w1 w2 ; do echo ">> node : cilium-$i <<"; kubectl get ciliumnodes cilium-$i -o yaml | yq .spec.ipam.pools.allocated ; echo; done
  >> node : cilium-ctr <<
  [
    {
      "cidrs": [
        "172.20.0.0/28",
        "172.20.0.16/28"
      ],
      "pool": "default"
    }
  ]

  >> node : cilium-w1 <<
  [
    {
      "cidrs": [
        "172.20.0.48/28",
        "172.20.0.80/28"
      ],
      "pool": "default"
    },
    {
      "cidrs": [
        "10.200.0.8/29"
      ],
      "pool": "no-engine"
    },
    {
      "cidrs": [
        "10.100.0.0/29",
        "10.100.0.16/29"  # 추가된 cidr
      ],
      "pool": "vac"
    }
  ]

  >> node : cilium-w2 <<
  [
    {
      "cidrs": [
        "172.20.0.32/28",
        "172.20.0.64/28"
      ],
      "pool": "default"
    },
    {
      "cidrs": [
        "10.200.0.0/29"
      ],
      "pool": "no-engine"
    },
    {
      "cidrs": [
        "10.100.0.24/29",   # 추가된 cidr
        "10.100.0.8/29"
      ],
      "pool": "vac"
    }
  ]
  ```

<br>

#### 3.4 Load Balaner / Egress IPAM

<p align=center><img src="./_image/lb_ipam.png" title="출처: ISOVALENT_BLOG" width="60%"></p>

- Cilium은 LoadBalancer/Ingress 유형의 Kubernetes 서비스에 클러스터 외부에 노출 할 External IP 주소를 직접 할당할 수 있다.

- 다른 CNI 같은 경우 Metal LB, AWS Load Balancer Controller와 같은 플러그인이 필요하지만 Cilium은 자체 기능을 통해 제공한다.

##### Load Balaner IP Pool 생성

- 쿠버네티스의 LoadBalancer, Ingress 리소스를 생성할 때 External IP에 할당할 IP Pool을 생성해야 한다.

  ```yaml
  # lb-ip-pool.yaml
  apiVersion: "cilium.io/v2alpha1"
  kind: CiliumLoadBalancerIPPool
  metadata:
    name: lb-ip-pool
  spec:
    cidrs:
      - cidr: "10.0.250.0/24"
  ```

- IP Pool 생성 후 Ingress Nginx 또는 ArgoCD와 같은 Ingress Controller, LoadBalancer Type의 리소스를 생성하면 자동으로 External IP가 할당된다.

  ```bash
  $ kubectl get svc -n ingress-nginx
  NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
  ingress-nginx-controller             LoadBalancer   10.96.201.226    10.0.250.1    80:31181/TCP,443:32179/TCP   16d
  ```

- IP 주소가 할당되고 난 다음에는 외부 클라이언트가 해당 IP 주소로 접속할 수 있도록 네트워크 설정이 추가되어야 한다.

- BGP를 이용하는 방식(ToR 네트워크 장치 필요)과 ARP를 이용하는 L2 방식이 있다.

- ARP 를 통한 L2 방식을 사용하기 위해서는 Cilium CNI의 옵션을 추가해야 한다.

  ```yaml

  ---
  # cilium-values.yaml
  kubeProxyReplacement: true
  autoDirectNodeRoutes: true
  l2announcements:
    enabled: true
  externalIPs:
    enabled: true
  ```

  ```bash
  helm upgrade cilium cilium/cilium -n kube-system --version 1.17.5 -f cilium-values.yaml
  ```

- helm으로 Cilium을 재배포한 후 설정이 반영되었는지 확인한다.

  ```bash
  $ cilium config view | grep enable-l2
  enable-l2-announcements                           true
  enable-l2-neigh-discovery                         true
  ```

- 그 다음 LoadBalancer/Ingress에 할당되는 External IP에 대한 ARP 요청에 응답할 수 있도록 `CiliumL2AnnouncementPolicy`를 생성한다.

  ```yaml
  # cilium-l2-announcement-policy.yaml
  apiVersion: "cilium.io/v2alpha1"
  kind: CiliumL2AnnouncementPolicy
  metadata:
    name: cilium-l2-announcement-policy
  spec:
    externalIPs: true
    loadBalancerIPs: true
  ```

  ```bash
  kubectl apply -f cilium-l2-announcement-policy.yaml
  ```

- 배포 후 LoadBalancer/Ingress Type 서비스에 할당된 External IP로 요청을 보내면 응답을 받을 수 있다.

<br>
