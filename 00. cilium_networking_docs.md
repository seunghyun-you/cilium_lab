**Table of Contents**
---
- [**Table of Contents**](#table-of-contents)
- [Cilium](#cilium)
  - [1. Cilium 구성요소](#1-cilium-구성요소)
    - [Cilium Operator](#cilium-operator)
    - [Cilium CNI Plug-in (Node Level 작업)](#cilium-cni-plug-in-node-level-작업)
    - [Cilium Agent (Kernel Level 작업, L3-4 계층)](#cilium-agent-kernel-level-작업-l3-4-계층)
    - [Envoy Proxy (UserSpace 작업, L7 계층)](#envoy-proxy-userspace-작업-l7-계층)
  - [2. 네트워크 구성 정보 : _cilium host, cilium net, cilium health_](#2-네트워크-구성-정보--cilium-host-cilium-net-cilium-health)
    - [cilium\_host](#cilium_host)
    - [cilium\_net](#cilium_net)
    - [cilium\_health (lxc\_health)](#cilium_health-lxc_health)
    - [lxcxxxx](#lxcxxxx)
  - [3. eBPF를 이용한 Packet Flow (eBPF Datapath)](#3-ebpf를-이용한-packet-flow-ebpf-datapath)
    - [3.1 같은 노드에 배치된 Pod 간의 통신 경로](#31-같은-노드에-배치된-pod-간의-통신-경로)
    - [3.2 Pod에서 외부로 나가는 트래픽 통신 경로](#32-pod에서-외부로-나가는-트래픽-통신-경로)
    - [3.3 Pod 내부로 들어오는 트래픽 통신 경로](#33-pod-내부로-들어오는-트래픽-통신-경로)
- [Cilium Networking](#cilium-networking)
  - [1. cilium이 서로 다른 노드에 있는 Pod를 연결하는 두 가지 방법: _Encapsulation(VxLAN, Geneve), Native/Direct_](#1-cilium이-서로-다른-노드에-있는-pod를-연결하는-두-가지-방법-encapsulationvxlan-geneve-nativedirect)
    - [1.1 Encapsulation Routing Mode (Default)](#11-encapsulation-routing-mode-default)
    - [1.2 Native Routing Mode](#12-native-routing-mode)
  - [2. 네트워크 엔드포인트(컨테이너/LB) IP 관리를 위한 IPAM (IP Address Management)](#2-네트워크-엔드포인트컨테이너lb-ip-관리를-위한-ipam-ip-address-management)
    - [2.1 Kubernetes Host Scope](#21-kubernetes-host-scope)
    - [2.2 Cluster Scope IPAM (Default)](#22-cluster-scope-ipam-default)
    - [2.3 Multi Pool](#23-multi-pool)
    - [2.4 Load Balaner / Egress IPAM](#24-load-balaner--egress-ipam)
  - [3. 클러스터 외부로 향하는 패킷 처리를 위한 Maquerading 처리](#3-클러스터-외부로-향하는-패킷-처리를-위한-maquerading-처리)
    - [Cluster와 같은 네트워크 대역에 있지만 Cluster에 Join 되지 않은 VM 과의 통신 테스트](#cluster와-같은-네트워크-대역에-있지만-cluster에-join-되지-않은-vm-과의-통신-테스트)
    - [Cluster 외부의 다른 네트워크 대역(10.0.0.0/16)에 있는 네트워크와의 통신 테스트](#cluster-외부의-다른-네트워크-대역1000016에-있는-네트워크와의-통신-테스트)
    - [Cluster 외부의 다른 네트워크 대역(10.0.0.0/16)과 Masquerade 없이 통신하도록 설정하는 방법](#cluster-외부의-다른-네트워크-대역1000016과-masquerade-없이-통신하도록-설정하는-방법)

## Cilium

<p align=center><img src="./_image/cilium.png" title="출처: Getting started with Cilium for Kubernetes networking and observability" width="60%"></p>

- Cilium은 eBPF 기술을 이용해서 쿠버네티스의 네트워크와 보안 기능을 구현한 쿠버네티스의 CNI Plugin 이다.

- eBPF는 리눅스 커널의 소스코드 변경 없이 커널 내부에서 샌드박스 프로그램을 실행시켜 커널의 기능을 효율적으로 확장시킬 수 있다. [ [BLOG](https://zerotay-blog.vercel.app/4.RESOURCE/KNOWLEDGE/OS/eBPF/) ]

<br>

### 1. Cilium 구성요소

<p align=center><img src="./_image/cilium_architecture_02.png" title="출처: https://velog.io/@baeyuna97/Cilium" width="50%"></p>

#### Cilium Operator

- Deployment로 배포되어 쿠버네티스 클러스터 단위에서 한 번씩 처리해야 하는 작업을 관리한다.

- Operator는 네트워킹 과정에 깊게 관여하지 않아 일시적인 중단에도 클러스터 동작에 영향을 미치지 않는다.

  - 설정에 따라서 IPAM Pool의 고갈 시 신규 IPAM Pool을 노드에 할당한다.

  - Operator의 장애로 신규 IPAM Pool 할당이 안될 경우 신규 Pod 생성이 실패한다.

#### Cilium CNI Plug-in (Node Level 작업)

- 노드에 Pod가 예약되거나 종료될 때 마다 노드에 구성된 kubelet에 의해 호출된다.

  - Operator, Agent, Envoy와 같이 컨테이너로 동작하지 않고 필요할 때만 kubelet의 자식 프로세스로 실행된다.

  - Binary 파일(`/opt/cni/bin/cilium-cni`)로 각 노드에서 관리된다.

- Pod 생성/삭제 시마다 네트워킹(NIC 설정, IP 할당/해제) 설정과 로드 밸런싱, 네트워크 정책 제공에 필요한 라우팅 경로를 업데이트한다.

#### Cilium Agent (Kernel Level 작업, L3-4 계층)

- 데몬셋으로 배포되어 각 노드에서 파드로 실행된다.

- 쿠버네티스 API 서버의 요청을 수신해 L3-4 계층의 네트워킹, 네트워크 정책 설정, 서비스 부하분산, 모니터링 등을 설정한다.

- 커널에서 컨테이너 네트워크 액세스를 제어하도록 eBPF 프로그램을 관리한다.

- eBPF 프로그램을 커널 내부에 로드하고, 파드의 네트워크 트래픽을 처리하며, 네트워크 정책을 시행한다.

  - Kernel에서 수행될 코드를 ByteCode로 Injection하여 패킷 필터링, 라우팅, 트래픽 모니터링 등을 수행한다.

  - 사용자가 네트워크 정책(L3-4 계층)을 통해 파드간의 트래픽을 제어할 수 있는데, 이 정책이 eBPF Program으로 변환하여 실행된다.

#### Envoy Proxy (UserSpace 작업, L7 계층)

- Cilium L7 계층 관련 기능(Ingress, Gateway API, L7 Network Policies, L7 Protocol Visibility)을 사용하는 경우, Envoy Pod를 이용해 L7 트래픽을 관리한다.

- 요청을 라우팅하고, 로드 밸런싱을 수행하며, 모니터링 및 로깅 기능을 제공한다.

> [!TIP]
>
> - L3-4 계층의 패킷은 Agent에서 관리하고, L7 계층의 패킷은 Envoy에서 관리한다.
> - Cilium은 MetalLB의 지원 없이 직접 LB, Ingress의 External IP 할당을 관리할 수 있다.
> - Envoy는 Ingress에 적용된 규칙에 맞춰 라우팅 하는 기능과 L7 계층 Network Policy를 구현하는데 사용된다.

<br>

### 2. 네트워크 구성 정보 : _<span style="font-size: smaller; color: Aquamarine;">cilium host, cilium net, cilium health</span>_

<p align=center><img src="./_image/cilium_interface.png" title="출처: Ctrip Network Architecture Evolution in the Cloud Computing Era" width="40%"></p>

#### cilium_host

- Cilium이 관리하는 호스트 네트워크 인터페이스

- 클러스터 내의 Pod와 외부 네트워크 간의 연결을 처리 (Pod가 외부 네트워크와 통신할 때 이 인터페이스 사용)

- Pod에서 나가는 트래픽과 외부에서 들어오는 트래픽을 처리하는 데 사용

- 외부에서 들어오는 패킷을 Pod로 전달하는 Reverse NAT 기능을 포함

#### cilium_net

- Cilium에서 관리하는 가상 네트워크 인터페이스

- Cilium의 eBPF 프로그램을 통해 네트워크 트래픽을 처리하고 제어

- Pod 간 통신 관리(보안 정책 적용, 패킷 필터링, 네트워크 성능 측정 등)를 위해 cilium_net 사용

- cilium_net 인터페이스는 각 Pod에 대해 생성되며, Pod의 IP 주소와 연결

- 이 인터페이스는 eBPF 프로그램이 패킷을 검사하고 처리할 수 있도록 해준다.

#### cilium_health (lxc_health)

- 컨테이너의 상태 확인에 사용하는 인터페이스

#### lxcxxxx

- 컨테이너에 할당되는 eth 인터페이스와 호스트의 인터페이스와 맵핑되는 가상 인터페이스

<br>

### 3. eBPF를 이용한 Packet Flow (eBPF Datapath)

#### 3.1 같은 노드에 배치된 Pod 간의 통신 경로

<hr style="border: none; border-top: 0.5px solid #ccc; width: 100%; margin: 1em 0;">

<p align=center><img src="./_image/packet_flow_endpoint_to_endpoint.png" title="출처: Cilium Documentation - Networking.eBPF_Datapath.Life_of_a_Packet" width="80%"></p>

- Egress, Ingress L7 정책이 적용된 경우 점선으로 표시된 항목들까지 통신 경로가 추가 된다.

- 별도의 L7 정책이 없다면 Pod간의 통신에는 `lxcxxxx` 인터페이스를 지나는 순간 커널의 TCX(Traffic Control eXpress) ingress/egress Hook 을 통해 eBPF 프로그램이 트리거 된다.

- 이 때 네트워크 트래픽은 커널 내부의 eBPF 프로그램에 의해서 정책이 평가되고, 평가 결과에 따라서 Flow가 결정된다.

- 실행되는 eBPF 프로그램(Component)명은 노란색 박스에 표시되는데, 이 프로그램들은 `cilium-agent` 내부에 들어가거나 Cilium GitHub 문서에서 확인할 수 있다. [ [docs](https://github.com/cilium/cilium/blob/main/bpf/bpf_lxc.c) ]

  ```bash
  root@cilium-ctr:/var/lib/cilium/bpf# ls -al
  total 336
  drwxr-xr-x 1 root root  4096 Jul 16 10:05 .
  drwxr-x--- 1 root root  4096 Jul 28 14:05 ..
  -rw-r--r-- 1 root root   420 Jul 16 10:05 COPYING
  -rw-r--r-- 1 root root  1296 Jul 16 10:05 LICENSE.BSD-2-Clause
  -rw-r--r-- 1 root root 18012 Jul 16 10:05 LICENSE.GPL-2.0
  -rw-r--r-- 1 root root 20261 Jul 16 10:05 Makefile
  -rw-r--r-- 1 root root  3533 Jul 16 10:05 Makefile.bpf
  -rw-r--r-- 1 root root  2945 Jul 16 10:05 bpf_alignchecker.c
  -rw-r--r-- 1 root root 58666 Jul 16 10:05 bpf_host.c
  -rw-r--r-- 1 root root 76153 Jul 16 10:05 bpf_lxc.c       # lxcxxx interface를 지날 때 트리거 되는 eBPF 프로그램
  -rw-r--r-- 1 root root  2797 Jul 16 10:05 bpf_network.c
  -rw-r--r-- 1 root root 25289 Jul 16 10:05 bpf_overlay.c
  -rw-r--r-- 1 root root 31334 Jul 16 10:05 bpf_sock.c
  -rw-r--r-- 1 root root  1424 Jul 16 10:05 bpf_wireguard.c
  -rw-r--r-- 1 root root  9064 Jul 16 10:05 bpf_xdp.c
  drwxr-xr-x 6 root root  4096 Jul 16 10:05 complexity-tests
  drwxr-xr-x 2 root root  4096 Jul 16 10:05 custom
  -rw-r--r-- 1 root root  1870 Jul 16 10:05 ep_config.h
  -rw-r--r-- 1 root root   517 Jul 16 10:05 filter_config.h
  drwxr-xr-x 1 root root  4096 Jul 16 10:05 include
  drwxr-xr-x 2 root root  4096 Jul 16 10:05 lib
  -rw-r--r-- 1 root root   404 Jul 16 10:05 netdev_config.h
  -rw-r--r-- 1 root root 10753 Jul 16 10:05 node_config.h
  drwxr-xr-x 4 root root  4096 Jul 16 10:05 tests
  ```

- Pod 마다 할당된 eBPF 프로그램의 특정 함수를 조회하는 방법은 다음과 같다.

  ```bash
  # pod 조회
  $ kubectl get po -owide
  NAME                      READY   STATUS    RESTARTS        AGE   IP             NODE         NOMINATED NODE   READINESS GATES
  curl-pod                  1/1     Running   1 (4h34m ago)   14h   172.20.2.244   cilium-ctr   <none>           <none>
  webpod-697b545f57-5fmfp   1/1     Running   0               14h   172.20.0.97    cilium-w1    <none>           <none>
  webpod-697b545f57-g9c5f   1/1     Running   0               14h   172.20.1.38    cilium-w2    <none>           <none>
  # ENDPOINT 정보 조회
  $ c0 endpoint list | grep 172.20.2.244
  ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])        IPv6   IPv4           STATUS   
            ENFORCEMENT        ENFORCEMENT
  4          Disabled           Disabled          47080      k8s:app=curl                              172.20.2.244   ready
  # Interface Name 조회
  $ c0 endpoint get 4 | grep -A11 networking
        "networking": {
          "addressing": [
            {
              "ipv4": "172.20.2.244",
              "ipv4-pool-name": "default"
            }
          ],
          "container-interface-name": "eth0",
          "host-mac": "12:73:95:0d:4e:56",
          "interface-index": 9,
          "interface-name": "lxc49adfa975abf",  # LXC 변수에 저장
          "mac": "4e:22:88:88:7d:20"
  # interface 변수 저장
  $ LXC=lxc49adfa975abf
  ```

- Pod의 Interface에 적용된 eBPF 프로그램을 조회하면 `cil_from_container`, `cil_to_container` 함수 이름을 확인할 수 있다.

  ```bash
  $ c0bpf net show | grep $LXC
  lxc49adfa975abf(9) tcx/ingress cil_from_container prog_id 1143 link_id 23 
  lxc49adfa975abf(9) tcx/egress cil_to_container prog_id 1149 link_id 24
  ```

- 이 함수는 bpf_lxc.c 파일에 선언되어 있는데 ciliu-agent에 접속 후 해당 함수(`cil_to_container`)를 조회하면 함수를 확인할 수 있다.

  - 함수 시작과 동시에 `trace`, `magic`, `identity`, `sec_label` 과 같은 변수를 초기화 한다.
  
  - `validate_ethertype()` 함수를 통해 Ethernet Frame의 Type을 확인하고, 유형이 맞지 않으면 DROP 시키는 코드를 반환한다.
  
  - `bpf_clear_meta()`함수로 전달 받은 패킷 컨텍스트를 초기화 후 새로운 정책 적용을 위한 준비를 한다.
  
  - `EBABLE_L7_LB` 가 정의되어 있는 경우 해당하는 정책으로 점프 시킨다.
  
  - `inherit_identity_from_host()` 함수를 통해 호스트의 정보를 가져와 `MARK_MAGIC_PROXY_INGRESS`, `MARK_MAGIC_PROXY_EGRESS` 인 경우 `send_trace_notify()` 함수를 실행한다.
  
  - 호스트의 방화벽이 활성화되어 있는 경우 `tail_call_policy()` 함수를 통해 호스트 방화벽 정책 적용 후 bpf_lxc로 돌아와 결과에 따라서 패킷을 처리한다.
  
  - Protocol의 종류(ARP, IPv4, IPv6)에 따라서 `tail_call_internal()`함수로 처리하거나 `CTX_ACT_OK` 값을 리턴하도록 구성되어 있다.

  ```bash
  $ grep -n -A160 "cil_to_container" /var/lib/cilium/bpf/bpf_lxc.c
  2335:int cil_to_container(struct __ctx_buff *ctx)
  2336-{
  2337-   enum trace_point trace = TRACE_FROM_STACK;
  2338-   __u32 magic, identity = 0;
  2339-   __u32 sec_label = SECLABEL;
  2340-   __s8 ext_err = 0;
  2341-   __u16 proto;
  2342-   int ret;
  2343-
  2344-   if (!validate_ethertype(ctx, &proto)) {
  2345-           ret = DROP_UNSUPPORTED_L2;
  2346-           goto out;
  2347-   }
  2348-
  2349-   bpf_clear_meta(ctx);
  2350-
  ...
  2442-
  2443-out:
  2444-   if (IS_ERR(ret))
  2445-           return send_drop_notify_ext(ctx, identity, sec_label, LXC_ID, ret,
  2446-                                       ext_err, CTX_ACT_DROP, METRIC_INGRESS);
  2447-
  2448-   return ret;
  2449-}
  2450-
  2451-BPF_LICENSE("Dual BSD/GPL");
  ```

#### 3.2 Pod에서 외부로 나가는 트래픽 통신 경로

<p align><img src="./_image/packet_flow_egress_from_endpoint.png" title="출처: Cilium Documentation - Networking.eBPF_Datapath.Life_of_a_Packet" width="60%"></p>

#### 3.3 Pod 내부로 들어오는 트래픽 통신 경로

<p align><img src="./_image/packet_flow_inress_from_endpoint.png" title="출처: Cilium Documentation - Networking.eBPF_Datapath.Life_of_a_Packet" width="60%"></p>

<br>

## Cilium Networking

- 기존의 전통적인(Standard) 방식의 CNI 기능은 `kube-proxy(iptables)`를 기반으로 동작한다.

- Cilium에서는 "kube-proxy 대체 모드"를 사용하면 kube-proxy 없이도 클러스터 네트워킹을 구현할 수 있다.

- eBPF 기반의 Cilium CNI는 `kube-proxy(iptables)`를 사용하는 환경보다 더 좋은 성능을 보여준다. [ [link](./_docs/Appendix%201.%20What's%20wrong%20with%20legacy%20iptables.md) ]

<br>

### 1. cilium이 서로 다른 노드에 있는 Pod를 연결하는 두 가지 방법: _<span style="font-size: smaller; color: Aquamarine;">Encapsulation(VxLAN, Geneve), Native/Direct</span>_

#### 1.1 Encapsulation Routing Mode (Default)

<p align=center><img src="./_image/encapsulation_routing_mode.png" title="출처: Kubernetes Networking & Cilium for Network Engineers - An Instruction Manual" width="70%"></p>

- UDP 기반 캡슐화 프로토콜인 VXLAN 또는 Geneve를 사용하여 모든 노드 간에 터널 메시가 생성된다. [ [link](./_docs/Appendix%202.%20VxLAN%20and%20Geneve.md) ]

- 노드 간 통신 트래픽은 모두 VXLAN 또는 Geneve을 통해서 캡슐화된다.

- Pod 네트워크는 노드 네트워크의 영향을 받지 않기 때문에 환경에 종속되지 않고 간단하게 구성할 수 있는 장점이 있다.

- 캡슐화를 통해 헤더가 추가되면서 패킷의 효율이 미미하게 떨어지는데, 최적의 네트워크 성능 보장이 필요한 경우 Native/Direct 모드가 적합하다.

<br>

#### 1.2 Native Routing Mode

<p align=center><img src="./_image/native_routing_mode.png" title="출처: Kubernetes Networking & Cilium for Network Engineers - An Instruction Manual" width="70%"></p>

- 캡슐화 기능 대신 Cilium의 네트워크 기능과 리눅스 커널의 라우팅 시스템을 이용해서 통신한다.

- 각 노드에는 Cilium Agent가 구성되고, Agent는 해당 노드의 Pod들에 대한 네트워크만 관리한다.

- 따라서 다른 노드로 향하는 트래픽은 리눅스 커널의 라우팅 시스템에 위임하여 처리된다.

  - 각 노드는 서로 다른 노드에 할당된 PodCIDR 정보를 인식하고, 커널 라우팅 테이블에 해당 노드로 향하는 경로가 추가된다.

  - L2 네트워크 프로토콜을 사용하는 경우 `auto-direct-node-routes` 옵션을 활성화하여 구성할 수 있다.

  - 또는, BGP 데몬을 활성화한 다음 서로의 라우팅 경로를 배포하도록 구성해야 한다.

<br>

### 2. 네트워크 엔드포인트(컨테이너/LB) IP 관리를 위한 IPAM (IP Address Management)

<p align=center><img src="./_image/ipam_mode.png" title="출처: Kubernetes Networking & Cilium for Network Engineers - An Instruction Manual" width="60%"></p>

- 네트워크 엔드포인트(컨테이너 등)에서 사용할 IP 주소를 할당하고 관리하는 역할을 한다.

- IPAM을 한 번 설정한 상태에서 모드를 변경하는 것은 권장되지 않는다. 새 IPAM 구성으로 새로운 쿠버네티스 클러스터를 생성하는 것이 좋다.

- 배포 환경, 사용자의 요구사항에 따라 다양하게 구성이 가능하도록 7개의 배포 모드를 지원한다. [ [docs](https://docs.cilium.io/en/stable/network/concepts/ipam/) ]

<br>

#### 2.1 Kubernetes Host Scope

<p align=center><img src="./_image/kubernetest_host_scope_ipam.png" title="출처: ISOVALENT_BLOG" width="60%"></p>

- `Kubernetes Controller Manager`가 Node에 할당한 PodCIDR을 사용한다.

- 이 모드는 간단하게 구현이 가능하지만 PodCIDR을 유연하게 관리하는 것은 제약이 많다.

  - 클러스터 전체에 하나의 PodCIDR 구성만 지원한다.

  - 클러스터의 PodCIDR이 고갈되었을 때 클러스터나 개별 노드에 PodCIDR을 추가하는 것이 불가능하다.

  - 초기 클러스터 배포 시 정확하게 노드별 IP 주소 계획을 수립해서 사용해야 한다.

<br>

#### 2.2 Cluster Scope IPAM (Default)

<p align=center><img src="./_image/cluster_scope_ipam.png" title="출처: ISOVALENT_BLOG" width="50%"></p>

- Kubernetes Host Scope와 동일하게 Node에 할당된 PodCIDR을 활용해 IP를 할당한다.

- Node에 할당된 PodCIDR을 할당하는 주체가 `Kubernetes Controller Manager`가 아닌 `Cilium Operator`가 된다.

- Cluster Scope IPAM의 장점은 여러 CIDR을 할당할 수 있다는 것이다.

- 하지만, 할당된 IP Pool의 주소 고갈 문제를 완전히 해결하지는 못하는 문제는 Kubernetes Host Scope와 동일하다.

<br>

#### 2.3 Multi Pool

<p align=center><img src="./_image/multi_pool_ipam.png" title="출처: ISOVALENT_BLOG" width="60%"></p>

- Pod IP Pool을 여러 개 생성한 다음 같은 노드에서 생성되는 Pod에게 서로 다른 IP Pool을 할당할 수 있다.

- `CiliumPodIPPool`을 생성해서 Pod IP Pool을 노드에 동적으로 추가할 수 있다.

<br>

#### 2.4 Load Balaner / Egress IPAM

<p align=center><img src="./_image/lb_ipam.png" title="출처: ISOVALENT_BLOG" width="60%"></p>

- Cilium은 LoadBalancer/Ingress 유형의 Kubernetes 서비스에 클러스터 외부에 노출 할 External IP 주소를 직접 할당할 수 있다.

- 다른 CNI 같은 경우 Metal LB, AWS Load Balancer Controller와 같은 플러그인이 필요하지만 Cilium은 자체 기능을 통해 제공한다.

<br>

### 3. 클러스터 외부로 향하는 패킷 처리를 위한 Maquerading 처리

<p align=center><img src="./_image/cilium_masquerade.png" title="출처: ISOVALENT_BLOG" width="60%"></p>

- Cluster 내부의 IP CIDR은 RFC1918 Private Address Block을 사용하기 때문에 그 자체만으로는 Public 통신이 제한된다.

- Node IP 주소를 이용해 Cluster 외부로 나가는 모든 트래픽의 Source IP를 자동으로 Masquerade 한다.

- 노드 간의 통신에는 Encapsulation, Native Routing 방식으로 통신하면서 Cluster를 떠나는 트래픽에 대한 Masquerade 만 지원된다.

- eBPF(default) 방식과 iptablse masquerade 방식을 지원한다.

- Cluster 외부 네트워크 중에 사내망의 특별 IP 대역 처럼 Masquerading 없이 통신하고자 하는 경우 `ip-masq-agent` 설정을 통해 Node IP로의 NAT 없이 통신할 수 있다.

  - helm을 통해 cilium을 생성할 때 `ip-masq-agent` 파라미터를 추해야 한다.

  - 세부 설정 값은 ConfigMap을 생성해서 관리한다.

  - Cluster 내부 리소스와 통신하려는 사내망의 네트워크에서도 Cluster 내부 리소스와 통신할 경로 값이 설정되어 있어야 한다.

  - Node 마다 Pod CIDR이 정의되어 있고 이 정보에 맞춰서 IP Routing 정보가 등록되어 있어야 한다.

  - 소규모 작업 환경에서는 사용자가 직접 라우팅 설정하는 것이 가능하지만 규모가 큰 경우 BGP를 이용해 동적으로 라우팅 정보를 전파해야 한다.

#### Cluster와 같은 네트워크 대역에 있지만 Cluster에 Join 되지 않은 VM 과의 통신 테스트

- 

#### Cluster 외부의 다른 네트워크 대역(10.0.0.0/16)에 있는 네트워크와의 통신 테스트

- router VM에 더미 인터페이스(loop1, loop2)를 만들어 두고, 해당 대역과의 통신을 테스트 한다.

#### Cluster 외부의 다른 네트워크 대역(10.0.0.0/16)과 Masquerade 없이 통신하도록 설정하는 방법